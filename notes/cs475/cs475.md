# CS 475

## Lecture 1 (2018-05-02)
### Prerequisites
- Basic linear algebra and calculus
- Scientific programming (Matlab, Python, Julia)
- Mathematical Maturity

### Outline
- Matrix Decompositions
	- LU. Gaussion Elimination
	- Cholesky
	- PDEs
	- Graph representation and ordering
	- Inverse problems
- Iterative method
	- Splitting
	- Steepset descent
	- Least-squares
	- Conjugate gradient
	- Projections
- Spectral Decomposition
	- QR decomposition
	- Reflection & Rotations
	- Eigen-value decomposition
	- Rayleigh Quotient
	- Subspace iteration
	- SVD (singular-value decomposition)

### Definitions
- **Linear algebra** studies the properties and operations of linear maps between 2 vector spaces
- **Computational linear algebra** studies the **implementation** of the operations on a modern computer

### Applications
- top 10 algs. in science & engineering for 20th century
	- Metropolis alg. for Monte Carlo (covered)
	- Simplex
	- Krylor Subspace (covered)
	- Decomposition algs (covered)
	- Fortran
	- QR (covered)
	- Quicksort
	- FFT (fast fourier transform)
	- Integer relation detection
	- Fast Muti-pole alg.
- Page Rank (Priority of a search result)
	- Assume n webpages on the internet with a probability vector **v**, pick an arbitrary webpage $$ S_{0} = j $$
		- toss a coin at time t
			- head: click a (random) link on webpage $$ S_{t} $$ to transfer to $$ S_{t+1} $$. If no link on $$ S_{t} $$, restart from $$ S_{0} $$
			- tail: go to  $$ S_{t+1} = k with probability v_{k} $$
		- in the long run, how often am I on each webpage k? (the frequency reflects the **importance** of a website; this is how google calculate pagerank)
	- Math
		- Initial distribution: start from webpage j
		- Transistion Matrix $$ P_{ij} = P_{r}(S_{t+1} = i | S_{t} = j) $$
			- column-stockestic matrix
		- Teleportation 
		- Biased Coin
		- Define $$ x_{t}(i) $$ as the prob of being on webpage i at time t, $$ \vec x_{t+1} = \alpha * P * \vec x_{t} + (1 - \alpha) * \vec V = [\alpha P + (1 - \alpha)\vec v 1^T ]\bar P * \vec x_{t} $$. $$ \vec x_{t} $$ is a prob vector that sums up to 1 i.e. $$ \vec x^T1 = 1 $$
	- Ergodic Theorem: $$ \frac{1}{T} \sum_{t=1}^{T}1(S_{t} = i) -> x(i) $$ where $$ \vec x = \bar P * \vec x $$
		- \vec x is an eigenvector of P, with eigenvalue 1
	- $$ (I - \alpha * \bar P) * \vec x = (1 - \alpha) * \vec v $$
	- $$ x = \bar P * x $$
- Semi-supervised learning / dimensionality reduction
- Graph-cut
- Information retrieval

### Notations
- **x**, **x$$_{t}$$**, $$x_{i}$$, $$x_{t,i}$$
- norms
	- LP norm
	- Triangule inequality: $$ ||x + y|| <= ||x|| + ||y|| $$

## Lecture 2 (2018-05-07)
### Objectives
- $$ \bar P \vec x = \vec x, \bar P = \alpha P + (1 - \alpha)\vec v 1^T $$
- $$ (I - \alpha P)\vec x = (1 - \alpha)\vec v $$

### Matlab
- j-th column of A - $$ A_{:,j} $$
- i-th row: $$ A_{i,:} $$

### Matrix Multiplication
- $$ A \in \Re^{m*n}, B \in \Re^{n*p}, C = A * B \in \Re^{m*p} $$ complexity O(mnp)
- Strassen's alg. $$ O(n^{2.89}) $$

### Matrix Inverse
- A is invertable if and only if $$ A \in \Re^{n*n} $$ and $$ A^{-1}*A = A * A^{-1} = I $$
- Thm (Sherman-Morrisen-Woodbury): $$ A\in\Re^{n*n}, B\in\Re^{m*m}, U\in\Re^{n*m}, V\in\Re^{n*m}, (A + uBV)^{-1} = A^{-1} - A^{-1}U(B^{-1}+V^TA^{-1}U)^{-1}V^TA^{-1} $$
	- In particular, $$ (A + \vec u \vec v)^{-1} = A^{-1} - \frac{1}{1+v^TA^{-1}u}A^{-1}uv^TA^{-1} $$
	- Proof: verify that the product is 1

### Goal: solve $$ A\vec x = \vec b, A \in \Re^{n*n} $$, A is invertible
- $$ \vec x = A^{-1}\vec b $$ is non-trivial for computer implementation
- e.g. $$ 	\begin{cases} 
				2x + y + 2z = 1 \\
				4x + 2y -z = 2 \\
				x + y + 2z = 3
			\end{cases} $$
- to solve a linear system, try to transform the matrix 
	$$ \begin{bmatrix}
		2 & 1 & 2 \\ 
		4 & 2 & -1 \\
		1 & 1 & 2
	\end{bmatrix} $$ to an upper triangular matrix
- Psedo code for solving lower triangular matrix $$ O(n^2) $$
	- 	```
		for n = 1, 2, ... n do
			$ y_i = (b_i - l^T_{i,1:(i-1)}y_{1:(i-1)})/l_{ii}; $
		```
	- **Problem** division of $$ l_{i,i} $$ could be 0. 
		- solution: add a small number to all diagonals to avoid such issues / pivoting
- During implementation, try to use vector instead of for-loop for calculation (avoid loop as much as possible)
- Gaussian Elimination psedo code
	- Input: $$ A \in \Re^{n*n}, \vec b \in \Re^{n*1} $$
	- output: in-place transform A to upper triangular matrix
	- code:
		```
		for j = 1,2 ... n-1 do
			for i = j + 1, ..., n do
			$$	a_{i,j} <- a_{i,j} / a_{j,j} $$
			$$	\vec a_{i,j+1:n} <- \vec a_{i,j+1:n} - a_{ij} $$
		```
	- still have the problem of division
		- how to guarantee that the diagnals are never zero? 
		- solution: compute the **determinent**: $$ det(A) = \sum_{\sigma}(-1)^{sgn(\sigma)} * \prod^{n}_{i=1} A_{i, \sigma(i)} $$

### Thm (LU factorization): let $$\widetilde{L}$$ and u be the strict lower triangular and upper triangular part of the output of GE, then A = L*u, $$ L = \widetilde{L} + I $$

### To solve linear system Ax = b
- note: don't use inverse ever!
- $$ \begin{aligned}
		A\vec x = \vec b, A &= LU \\
 		LU\vec x &= \vec b \\
		L\vec y &= \vec b, U\vec x = \vec y
\end{aligned} $$
- Steps:
	- run GE on A and b
	- forward solve to get y from Ly = b
	- backward solve to get x from Ux = y
- time complexity is $$ O(n^3) $$

## Lecture 22 Final Review

### LU factorization
- pay attention to matrix dimensions (has to be square matrix)
- A = LU, L is lower-tri with diag all 1's, U is upper tri
- **Cholesky** if A is p.d. & symmetric, $$ A = LL^T $$ (L's diag is no longer 1's)
- Linear system **Ax = b** steps
	1. [L, U] = lu(A)
	2. Ly = b (forward solve)
	3. Ux = y (backward solve)
- Sparsity (banded matrix)

### Finite difference approximation
- Checkerboard ordering: specify the order of variables in a matrix go to the vector

### Graph Representation / ordering
- CMK
- RCMK
- Minimum ordering

### Iterative algorithms
- Inverse problem (e.g. try to recover a signal; only observed part of corrupted signal)
- Splitting algorithms
	- Jacobi
	- GS
	- SOR
	- Convergence: converge if spectral radius ($$ max{|\lambda|} $$) of G < 1, where G is $$M^{-1}N$$ (iteration matrix)
- Steepest descent: $$ x = x - \eta * f'(x) $$
	- Normal equation: $$ A^T(Ax - b) = 0 $$
		- any minimizer satisfies normal equation
- Conjugate gradient
	- instead of ignoring all previous updates like steepest descent, memorize all previous steps
	- at most n iterations
- Projection algorithm
	

### QR (full, reduced)
- Gram-schmidt (classic vs modified)
	- triangular orthogonalization
- Householder

### Eigen-decomposition
- A has to be sqaure & symmetric
- each iteration takes $$ O(n^3) $$ times, but number of iterations is small
- similarity transform; do not change eigenvalues
- methods
	- power
	- inverse
	- rayleigh-quotient
https://github.com/FleetCarma/CS-Challenge.git
### SVD
- by hand
	- $$ AA^T / A^TA $$
	- [0 A^T; A 0]
- E-Y: Best rank k approximation is $$ U\Sigma_kV^T $$
- Bi-diagonalization $$ A = UBV^T $$


## Final Review
### Solving linear System
- GE, LU, Cholesky
	- Optimization
		- p.d. matrix, (p,q)-banded matrix, diagonally dominant matrix
		- level-set ordering on matrix: CMK, RCMK, minimum ordering
- Iterative methods (Splitting)
	- Richardson
	- Jacobi
	- Gauss-Siedel
- Least Squares $$ ||Ax - b||^2_2 $$
	- $$ A^TA, x = A^+b + ker(A) $$
	- Steepest descent
	- Orthogonalization 
		- GS / MGS
		- Householder reflection
		- Givens rotation
- Conjugate gradient $$ S = A^TA, c = -A^Tb $$

### Eigenvalue

### SVD

### Other topics
- Inverse Problem
	- Regularization term examples 
		- L2-norm $$||w||^2_2$$
		- L1-norm $$||w||_1$$
		- Laplacian $$ ||\nabla{w}||^2_2 $$
		- Total-variation $$ ||\nabla{w}||_1 $$


