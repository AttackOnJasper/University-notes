# Week 1
- Data Curation
	- the active and on-going management of data through its lifecycle of interest and usefulness to scholarship, science, and education
- curation activities and policies enable data discovery and retrieval, maintain data quality and add value, and provide for re-use over time
- Data science can be thought of dividing into two areas
	1. Data curation
		- Ensure data is available, reliable, and usable
	1. Data analytics

## What is Data Science
- Defnitions
	1. Introduction to Data Science, Jeffrey Stanton
		- Work concerned with the collection, preparation, analysis, visualization, management and preservation of large collections of information.
		- data scientists play active roles in four related areas
			1. data architecture
			1. data acquisition
			1. data analysis 
			1. data archiving
		- Data science is an applied activity, and data scientists serve the needs and solve the problems of data users
	1. StatSNSF
		- the science of planning for acquisition, management, analysis of, inference and discovery from data
	1. Our definition
		- Data science is concerned with all aspects of the creation, management, analysis, and communication of data focusing particularly on the application of computational methods to digital data
- Goal: extracting useful knowledge from data

## What is Data Curation
- Definition
	- data curation is the active and ongoing management of data through its lifecycle of interest and usefulness to scholarship, science, and education.
- Curation activities enable data discovery and retrieval, maintain quality, add value, and provide for re-use over time.


## Curational Activities and Objectives
1. Collection
	- Collection and acquisition of data
	- e.g. instrument calibration, protocols, procedures, documenting collection activity
1. Organization
	- Employ an appropriate data model and use appropriate standards
	- e.g. schema (datatypes, constraints, audit history), abstraction and indirection, standards
1. Storage
	- Support reliable and effective storage
	- Aspects: reliability, security, and access
1. Preservation
	- Ensure that data will be understandable and useable in the future
	- e.g. documentation of syntax and semantics
1. Discoverability
	- Support the ability to search for and locate relevant data
	- e.g. Develop metadata to support searching / ranking
1. Access
	- Support the ability to retrieve and distribute data
	- e.g. efficient retrieval and distribution, metadata for file descriptions, control access
1. Workflow
	- Support the ability to systematize work with data
	- e.g. (potential automatic generated) documentation of module and workflow execution, scripts
1. Identification
	- Support the ability to identify, authenticate, and validate data
	- including 
		1. identification methods
		1. deciding what to identify (entities and datatypes)
		1. version control
		1. authentication
		1. validation 
1. Integration
	- Support integration of data from different sources using different data models
	- including
		1. Accommodation of variations in syntax and data element semantics of the data from different sources
		1. schema alignment
		1. Integration strategy documentation
1. Reformatting
	- Support reformatting for use by different tools or to match new format standards
1. Reproducibility
	- Support ability to reproduce results, ensuring scientific validity and reliability
	- Including
		1. Documenting data collection, management, processing, and analaysis
1. Sharing
	- Support sharing data between researchers, teams, and institutions
	- Need to take care of 
		1. formats & documentation
		1. concerns of misuse (e.g. by policies)
1. Communication
	- Support representation, publishing, and visualizations that provide insight.
1. Provenance
	- Support identifying what inputs, calculations, and actions are responsible for data values
	- Including
		1. understanding how data values are generated / derived
1. Modification
	- Support management of corrections and updates
	- Including
		- Changes overtime can be tracked and audited
1. Compliance
 	- Ensure compliance to legal, regulatory, and local policy requirements
	- Including
		1. IP rights
		1. Privacy
1. Security
	- Ensure that data is secure from tampering or inappropriate access and distribution
	- Including
		1. Access control
		1. User identity and privileges
		1. Data identity
		1. Authentication
		1. Validation


## Methods of curatorial action
1. Analysis
1. Documentation
1. System design and implementation
1. Policy
1. Process




# Week 2: Relations

## Data models
- Components
	1. Structure
	1. Things - values, labels, entities, relationships
	1. Constraints - types, grammar, enum, cardinality (one-to-one, one-to-many, and many-to-many)

### Types
1. Relational
	- attribute-value pair
1. Tree
	- hierarchical structure
1. ER
	- abstraction, representing the things and relationships of a domain.

## Storage repesentation problems
- Problems
	1. Programs and users are interacting directly with storage
	1. intrinsic nature of the information is often not reflected in the management systems
- Impacts
	1. Failure of data independence
		1. changes needed when the storage method changes
		1. changes needed when new kinds of data need to be represented


## Relational model
- Idea
	- Conceptualize data as relations (tables) and then map those relations to whatever storage methods are being used
- Fundamental principles of data organization
	1. Abstraction
		- abstracts away from the transient and varying details of storage and processing and focuses on the essential features of the data itself.
	1. Indirection
		- Relational data management systems do not directly interact with the stored data representations, instead they interact indirectly with the stored data representations, via the relational representation that is mapped to the actual storage representation

### Details
- Keys
	1. Primary key
	1. Foreign key
- Schema
	- a general term for a specification of how data is or will be organized
		- may also specify: vocabulary, syntax, data types, attributes, value ranges, etc.
	- e.g. Swagger, XSD, OpenAPI
	- Schema for relation
		- e.g. **Table-name** (Id (underscore for PK), attribute1, attribute2)
- Functional dependencies
	- also referred as a redundancy - updating one field would also need to update another field together
	- can be fixed via normalization
		- create a separate table to store mappings between the redundant data
- **Abstraction** implemented with **indirection** supports **data independence**




# Week 3: Tree and XML
- Most of info is not stored in database (i.e. relations) but in documents
- Why it is a challenge to represent documents with a relational model?
	- Representing the structure of a document in a set of relations is not obvious or natural

## Problem
- Interaction with text is immediately and directly via storage and processing methods
- i.e. No abstraction of storage and processing methods
- Impact
	1. Operational burden
	1. Lack of functionality
	1. Lack of data independence (i.e. no abstraction / indirection)

## Solution 1: Descriptive Markup
- Descriptive markup is used to (directly) indicate the logical components of a document
- Why it helps on data curation?
	1. More efficient less error-prone management of processing (layout)
	1. Supports conversion from one text processing system to another.
	1. Provides meaningful fields for analysis and retrieval
	1. Support preservation
- Examples
	1. A macro is defined to abbreviate formatting commands
	1. Improvement: A macro is defined to identify the logical component of the text itself not the the intended processing, or the appearance of the that component
- Descriptive markup describes the logical components of documents. It does not specifiy processing.

## Solution 2: Trees
- OHCO (Ordered Hierarchy of Content Objects) model
	- content objects. e.g. chapters, title, sentences
	- hierarchy: sentences inside paragraphs, paragraphs inside sections
	- ordered: objects proceed or follow one another
- Tree
	- a directed acyclic graph with ordered branches and all nodes except one having exactly one parent
- Tree Model
	- combines the tree data structure with descriptive markup node labels
	- why it works?
		1. abstraction
			- separate from storage and processing	
		1. indirection
			- indirect relationship to storage and processing
- Differences in abstractions between tree and relational model
	- In relational model the focus is on abstracting away from storage, while in the tree model the focus is on abstracting away from processing.

## XML
- Typically implements the tree model, which can be used to 
	1. store text data
	1. seralize data stored natively in other data models (i.e. interchange, import, and preservation)
- Features
	1. uses a defined set of delimiters with arbitrary element names and attribute value pairs to nest spans of text
- A well-formed XML doc 
	1. fits a formal grammer to ensure the doc can be parsed as a tree by XML parser
	1. Only one element is not the child of any element
	1. No overlapping elements
	1. Every start tag has a corresponding end tag
- A valid XML doc
	1. is well-formed
	1. confrms to the schema declarations on grammar & constraints
- 2 Main things in XML world
	1. Schema
		- defines a kind of a document via a markup language for document structures by specifying its vocabulary (i.e. the tags) and syntax
		- Functionality
			1. Specifying what elements may and must occur and where they may and must occur.
			1. Attributes and data types
		- Examples	
			1. DTD (Document Type Definition) 
				- Based on BNF (Backus Naur Form) grammars
			1. XSD (XML Schema Definition Language)
			1. RelaxNG
	1. Document Instances
		- A particular well-formed XML doc
 - XML Processing





# Week 4: Ontologies

## Problem: connecting data to information
- The same domain of kinds of objects, relationships, properties can be represented via different data models types and different schemas
- In the formal theory of the relational model, an “attribute” is the name of a domain
- The understanding of the domain needs to be captured:
	- Benefits of capturing the understanding of domain
		1. Guides initial schema development
		1. Constant across schema variations and revisions (within a model type) 
		1. Constant across schemas from different model types
	- Thus we need an independent neutral way to describe the domain of interest.
- The problem: we lack a shared framework that could explicitly and formally map the relevant features in the domain of interest to the relation or tree schemas for data


## Solution: Ontology
- Ontology
	- an explicit specification of a conceptualization of a domain
		- A conceptualization is an abstract, simplified view of the world that we wish to represent for some purpose.
- Peter Chen 1976
	- Conceptualize your domain of interest in terms of its things, relationships, etc., and then map that conceptualization to whatever logical model schemas are being used
- Conceptual Schema
	- a description of the structure of the domain, a specification of the real world things, attributes, and relationships, in that domain.
- Benefits
	1. support the creation of logical schemas - can be auto generated 
- RDF (Resource Description Framework)
	- A simple model for predication; can be serialized in different languages
	- General model for describing that some thing has a particular property with a particular value.
	- RDF triple (see more in below sections)
		- simple dyadic predication
- Examples
	1. ER diagrams
	1. RDFS (RDF Schema)
		- Basic schema concepts that can be used to define schemas for RDF instances
	1. OWL (Web Ontology Language)
		- General schema concepts for more advanced schemas

## FRBR Ontology
- FRBR: Functional Requirements for Bibliographic Records

### Creating Ontologies
1. Determining entities 
	- Isolate the kinds (types) objects (entities) that are of interest.
1. Identify attributes
	- Attribute assignments
		- Each entity is assigned a distinctive set of attributes
		- Each attribute assignments are disjoint
1. Identify relationships between entities
	- types
		1. "subclass" relationship
		1. "named" relationship
	- examples: exemplifies, embodies, realizes, adaption, translation, abridgement, part of

## Implementing Ontologies via RDFS
- RDF can be expressed as a graph / serialized in XML
- RDF Data Model
	- The basic abstract structure in RDF is a triple
		1. Subject
		1. Predicate 
		1. Object
	- The RDF graph is a collection of above triples
- RDFS
	- a schema language for defining RDF vocabularies
	- itself uses RDF triples as its syntax
	- can connect an instance the the schema


# Week 5: Data integration

## Data cleaning
- Data integration
- Approaches to integration
	1. Federation
		- Leaving datasets that are to be integrated in their original formats but mapping their schemas to a single meta-schema
	1. Derivation
		- derive a single dataset and a single schema

## Heterogeneity
- Kinds
	1. Encoding
	1. Syntax
		- different data description languages for the same model type e.g. XML / JSON 
	1. Model
		- different model types e.g. relation vs tree vs ER 
	1. Representational
		- Different modeling choices within a model type e.g. xml - element or attribute?
	1. Semantic
		- Synonyms, homonyms, missing relationships, different but related concepts
		- most challenging
	1. Processing
	1. Policy


## Schema Integration
- the process of merging several conceptual schemas into a global conceptual schema that represents all the requirements of the application
- usually takes place at the conceptual level or logical level
- 2 kinds of integration problems
	1. Representational
	1. Semantic
		1. synonyms
		1. homonyms
		1. conceptual overlaps
		1. generality variation



# Week 6: Data concepts

## Dataset Features
1. Content
	- Observation, DataObject, Value, Data, File and Records
1. Grouping
	- Aggregation, Set, Collection, Organization and Knowledge base
1. Relatedness
	- Common theme, Integratedness, Commonly structured and Logic of a collection
1. Purpose
	- analysis, evidence, explaining, being explained

## Identity Problems
- Identity Problems in data curation
	1. Archiving
	1. Preservation
		- e.g. what's the preserved data format
	1. Security
		- e.g. If dataset has been tampered
	1. Authentication
		- e.g. If this is the data we think it is
	1. Reproducibility
		- Does this XML file have the same information as that JSON file?
	1. Provenance
		- e.g. if datasets were derived from the same data
	1. Conversions
- Representation levels
	- levels of encodings, structures, formats, serialization etc
		- e.g. UTF-8, RDF triples, N3 serialization


## Roles vs types
- Roles
	- A property is a role if and only if anything that has it could exist and fail to have it
	- e.g. being a student is a role because persons are students only in virtue of particular contingent circumstances
- Types 
	- A property is rigid if and only if nothing that has it could exist and fail to have it
	- e.g. being a person is a type because persons are persons regardless of contingent circumstances


## The basic representation model (Ontology)
- Propositional Content (Semantic level)
	- Is expressed by
- Symbol Structure (Encoding levels)
	- Is encoded by itself
	- Is inscribed by (inscription: a physical instantiation of an encoding)
- Patterned Matter and Energy (instantiation level)
	- represents an instance
	- interpretation of the term: a physical matter in energy, patterned in some way or other

### Interpretative Frames
- Standards for defining specific expression, encodings, and inscriptions

### Side note
- two different symbol structures can express the same proposition
- two different symbol structures can encode the same symbol structure (due to recursion)


## What's data?
- Data are **propositions asserted as evidence**
	- Entity type for data: propositional content
- Data is not a type of thing, it is a role
- Data is socially constructed
- Propositions are data only in particular social circumstances.
- Data is relative
	- The same proposition can be data supporting a claim in one circumstance, and a claim being supported by data in another circumstance




# Week 7: Metadata

## What's metadata
- initial definition: data about data
- better definition: structured data about an object that supports functions associated with the designated object
- Classification by function
	1. Descriptive
	1. Administrative
		1. Technical
		1. Preservation
		1. Rights
	1. Structural
- Distinction about whether data is metadata is typically pragmatic, but there's no clear hard distinction

## Metadata Schemas
- A set of data elements, with specified meanings for supporting metadata statements in particular contexts
- vocabulary independent
	- Metadata schemas are sometimes purely conceptual
	- benefits
		1. gracefully support different languages
		1. take advantage of common meanings for common words
	- The benefts (of indirection) may be outweighed by the advantages of providing a controlled term vocabulary
- syntax/serialization independent
	- metadata schemas need not specify any particular syntax for applying specified concepts
	- Different serialization strategies are needed in different contexts
- Example
	1. Dublin Core
	1. EXIF

## Metadata Ambiguities
- Metadata is often used with ambiguous semantics
	- e.g. a digital image of paintings could have a format of JPEG or a format of oil painting - this is hard to be understood by machines



# Week 8: Data Practices

## Data practices
An empirical view of what people creating, analyzing, and managing data actually do. (or would do) so that we can improve efficiency and reliability

## Data Sharing
- important and valuable to communities beyond the developing community
- Challenges from sharing side
	1. Data misuse concerns
	1. Legal / ethical issues
	1. No incentives
	1. Laborious effort


## Data Reuse
- definitions
	1. the use of data collected for one purpose to study a new problem
	1. the use of data collected by one technical community and being used by another technical community
- Comparing to data sharing, data reuse focuses on 
	- How communities use (or why they don’t use) relevant data that they did not produce and for which they are not the intended consumer

## GDPR (General Data Protection Regulation)
- Aspects
	1. Security
		- Capability to detect a data breach
		- Control and mechanisms to detect and prevent attacks
	1. Data subject rights
		- Access requests about identifiers would be free
	1. Consent
		- Need to ask customers' consent to get their data
	1. Privacy

## Data Anonymization

### Data Sanitization
- Removing or obfuscating information that is contained in stored or distributed data
- Why bother
	1. ensuring conformance with the relevant laws
	1. minimizing civil liabilities
	1. safe-guarding trade secrets
	1. declassifying national security documents
	1. protecting individual identities.

### Data Privacy
- freedom from observation or from the distribution of collected personal information
- 2 conflicting trends
	1. the rapidly increasing importance of fine-grained digital data to science, business, and government
	1. increasing concern about protecting personal data in digital format
- Consequently, there has been a rapid establishment of more stringent and more complex norms, both for regulating collection and distribution, and for storage and internal use

### Anonymization and PII
- Personal Identifiable Information (PII) - Information that can be used to identify a particular individual
- K-Anonymity
	- A released dataset is believed to be k-anonymous if the information of each individual is indistinguishable from information of at least k-1 other individuals in the dataset.

### Techniques
1. Masking: Masking replaces meaningful data with arbitrary symbols or with codes
	1. Redacting
	1. Replacement with fabricated data similar in format or data datatype (tokenization)
	1. Encoding
1. Shuffling: the values within a column are redistributed, breaking the connection between a data value and a person
	- Benefits
		1. the occurrence, number, and ratios of values remain in the data
			- Can be used to get group data without revealing individual
1. Perturbation: numerical data is rounded and some additional slight numerical variation (“noise”) is added
1. Generalization: replaces specific data with more general data.
1. Combinations: Anonymization techniques can be combined, to realize adjustments in the protection vs information tradeoff.



## Data Integrity
- refers to the completeness and consistency in quality of data over its life-cycle, across different platforms and formats

### XML Signature
- An XML signature or XMLDSIG is an xml syntax for digital signatures defined by the "W3C recommendation XML Signature Syntax and Processing"
- Purpose
	1. provide integrity, authentication for both message and signer services for data of any type
- Use cases
	1. sign including but not limited to XML documents (other formats: HTML, JPEGs etc.)
- Features
	1. sign certain portions of XML tree rather than the entire document
- How it works?
	1. Apply exclusive canonical algorithm to treat white spaces and non utf-8 characters
	1. the document is then hashed using a suitable hashing algorithm like SHA or MD5 to create a hash value. This serves as the Digest Value in the blank XML signature syntax
		- Digest value binds a resource’s content to the signer’s key and the method (Hash Algorithm) which we used to create this digest value is specified in the digest method element of the signature


### Hashing
- SHA1
	- SHA: Secure Hash Algorithm
	- 160-bit message digest
- MD5
	- MD: Message Digest
	- 128-bit message digest
	- faster than SHA1
	- weaker against brute force attack
	- vulnerable against cryptanalysis





# Week 9: Preservation
- To ensure that the data can be used effectively and reliably in the future despite lots of changes over time
- Focuses our attention on understanding and credibility, not physical objects


## What's data preservation
- Digital Preservation
	- Best simple definition: Ensuring reliable communication with the future
- More specifically, future users should
	1. have possession of physical media and encodings
	1. recognize the originally intended propositional content
	1. can verify the propositional content is the intended content

## Challenges
1. Obsolescence
1. Physical Threats
1. Context (medadata) could be lost 

## Preservation challenges is similar to integration challenges
- Preservation Goals
	1. Viable
	1. Renderable - can be correctly viewed, processed, or executed.
	1. Understandable
	1. Authenticatable - can be correctly determined that the data is what it purports it be.
	1. Identifiable
- Communication involves communicating about
	- encoding (bits, bytes, UTF) through documentation
	- syntax via schema and documentation
	- propositions via e.g. ontology
- Natural language prose


## Strategies
1. Replication
1. Migration
	- Keep updating your data to new formats, as needed
1. Emulation
	- Maintain software that emulates the original processing
1. Normalization
	- Normalization vs Migration
		- Both need transformations
		- Migration creates a chain of data sets with the same data in different formats; normalization is a hub and spokes model

## Preservation Standards
1. OAIS (open archival info system)
	- high level data archiving standard
1. PREMIS
	- An international standard for metadata to support the preservation of digital objects.


### PDS (Planetary Data System)
- 6 data centers across US
- Archiving uses OAIS model



# Week 10: Data Identity

## Identification Overview
- Identity Problems (Week6)
- The fundamental challenge for determining identity at any level (propositions, description, encodings) is that the relationship of every level of abstraction (propositions, description, encoding-1, encoding-2, . . . encoding-n) is related to 1:M

### Identifiers
- Responsibilities
	1. Enable discovery and reuse of releveant data sets
	1. Support management of data sets
	1. Support workflow and provenance tracking
	1. Promotes transparency and reproducibility
	1. Give credit to data producers
- Popular identifiers: FORCE11, Data Citation Priciples, The Dataverse Project

## What to identify
- All proposition & symbol structures of the data
	- Including
		1. Semantic level (proposition)
		1. Syntax level (symbol)
		1. Encoding levels (symbol)
- No need to identify instantiation level

## How to identify
- Bottom-up process
	- Identify the bottem bitstream encoding level to indirectly identify higher encoding levels, syntax, or propositional content

### Identifier & Change
- The digital object itself is not changing (even though we say it is). However its relationships are changing, and new things are appearing and playing new roles
	- Digital objects do not undergo intrinsic (nonrelational) change; instead, they undergo relational change
- Use identifiers to identify, not to describe
	- Use non-descriptive (“opaque”) identifiers to identify digital objects (e.g. via adding label)

## Canonicalization
- A central strategy for determining identity of representation or (a reasonable proxy for) propositional content
	- a technique for determining representational identity
	- a reasonable proxy for propositional identity
- Steps
	1. Convert to a single character encoding and normalize line ends.
	2. Remove all comments, tabs, non-significant spaces, etc.
	3. Propagate all attribute defaults indicated in the schema to the elements themselves
	4. Put attribute/value pairs on elements in alpha order
	5. Expand all character references
	6. Remove any internal schema or declarations.
	7. Now test to see if character sequences are identical.
- a fixity checksum on files in canonical normal form is the best shot at sameness of propositional content.




# Week 11: Standards
- an alternative to standardizing data description languages
	- Standardize how alternative languages can be defined

## Standards Org
- W3
	- Focuses on creating standards for web devices, web applications and web architecture
- DCMI
	- Focuses on creating standards for metadata

## Compatibility
- Issues of changing standards / conversions 
	1. expensive
	1. lose information
	1. break existing applications
- Conformances
	1. data set conformance
	1. processor conformance
- Compatibility relationships





# Week 12: Transformations and Workflows

## Workflow
- why data curation is concerned with data set transformations
	1. Data-intensive science often involves data set transformations, and a major part of data curation is managing, documenting, versioning, and otherwise supporting these transformations that are central to the research process.
	1. Data curation activities themselves often involve transformation — for instance: format conversion and updates, preservation by migration, use of canonical normal forms, data cleaning, data reduction, etc.

## Provenance
- General definition: A record of ownership of a work of art or an antique, used as a guide to authenticity or quality
- Computational provenance definition: the sources of information, including entities and processes, involved in producing or delivering an artifact
- Heart of provenance
	1. Data being used
	1. Calculations being performed
- Levels of provenance
	1. Black-box
	1. White-box
	1. Grey-box
		- Identification of datasets and high level processes is available
- Prospective provenance
	- a correct specification the workflow scenario
- Retrospective provenance
	- generated data on the execution of the workflow scenario


## Workflow Systems
- Examples
	1. MapReduce
	1. Scripts
	1. Kepler


## Provenance Standards




# Week 13: Scientific Communication

## Scientific Communication
- Challenge: too many papers to read due to the use of tools / data science
- Solutions
	1. Text mining
	1. Tools for strategic reading
- Specific approaches for supporting scientific publications
	1. annotating articles with standardized structured terminologies from scientific ontologies
	1. developing tools that, making use of the above annotations, support searching, navigation, visualization, extraction, and integration with other tools, systems, and datasets.
	1. promoting the use of standard document formats for scientific publications
- Areas of standards
	1. Domain ontologies
		- most in need of ongoing development




# Week 15: Regulations

## Policies
- Policies' grammatical moods are imperative (or prescriptive)
- Types of data policies
	1. Regulatory
		- Conformance is not optional if you wish to participate in the activity in question, and sometime conformance is required regardless.
	1. Exhortative
		- Advice

## Existing Policies
- NIH
	1. Sharing
	1. Responsible Conduct of research
	1. IP policy
- NSF (National Science Foundation)
	1. Sharing
	1. DMP - Data Management Plan
- Archive
	- Easy to support a fixity checksum

## Privacy
- Privacy protection techniques
	1. Consent
	1. Anonymization
		1. Remove values
		1. Redaction
		1. Aggregation
		1. Statistical disclosure control



# XML Overview

## DTD (Document Type Definition)
- defines the structure and the legal elements and attributes of an XML document.
- XML building blocks
	1. Elements (tags)
	1. Attributes
		- provides extra info about the elements
		- always placed inside the opening tag of an element
		- key value pairs
	1. Entities
		- Some characters have a special meaning in XML
	1. PCDATA
		- Parsed character data
	1. CDATA
		- Character data

### Examples
- Inside XML
```
<!DOCTYPE note [
<!ELEMENT note (to,from,heading,body)>
<!ELEMENT to (#PCDATA)>
<!ELEMENT from (#PCDATA)>
<!ELEMENT heading (#PCDATA)>
<!ELEMENT body (#PCDATA)>
]>
```

## XSD (XML Schema Definition)
- describes the structure of an XML doc
```
<xs:element name="note">
  <xs:complexType>
    <xs:sequence>
      <xs:element name="to" type="xs:string"/>
      <xs:element name="from" type="xs:string"/>
      <xs:element name="heading" type="xs:string"/>
      <xs:element name="body" type="xs:string"/>
    </xs:sequence>
  </xs:complexType>
</xs:element>
```
