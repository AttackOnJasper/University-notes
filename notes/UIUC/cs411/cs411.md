# Week 1: Conceptual Modelling
- Steps to build a DB app
	1. Understand domain / users
	1. Conceptual data modeling e.g. ER
	1. Physical data modeling
	1. Create DB and develop app 
- Benefits of conceptual modeling
	1. focus on high-level description
	1. Use natural expression 
		- e.g. can sketch diagrams; no need to write in computer languages
	1. Bridge to physical modeling

## ER
- ER diagram is the diagram generated by ER modelling
- ER modelling: a diagram language to specify:
	1. What information the database must hold.
	1. The components of that information.
	1. The relationships among the components of that information.

### Components
1. Entity sets
1. Attribute
	- a property of an entity
	- Each attribute has a simple value
1. Relationships
	- Relates two or more entity sets for their interaction
	- Cardinality (or degree): Number of entity sets involved.
		1. Binary
		1. Multiway
	- A relationship can have attributes as well
	- Multiplicity of Binary Relationships
		1. Many-one
			- Zero-or-one entity at the "one" side can be related from an entity at the other side
			- Zero-or-more entities at the "many" side can be related from an entity at the other side
			- One side is denoted as an arrow
		1. One-one
		1. Many-many 
		1. Exactly one
			- round arrow
	- Relationship role
		- an entity set may be involved multiple times in a relationship
		- It is possible for an entity set to relate to itself
		- we can label each involvement with a role
	- IsA relationship
		- Subclass
	- Constraints
		- Types
			1. Key - unique id
			1. Referential integrity
				- A referenced entity mush exist
				- i.e. Exactly one relationship
			1. Domain constraints
			1. General constraints


### ER vs UML
- UML Allows Binary Associations Only
	 - K-way Can Be Expressed as K Binaries


# MP1
### Setup MySQL
- Start MySQL server: `sudo /usr/local/mysql/support-files/mysql.server start`


# Week 2: Physical data modelling

## Relational model
- Definitions
	1. Each entry in the relation is called as a tuple / record / row
	1. Each column in the relation is also called as an attribute / field
	1. Each attribute has a data type called domain
		- The domain must be atomic
		- e.g. integer, string, real
	1. Schema
		1. Relation schema
			- Relation name, attribute names, and the domains
		1. Database schema
			- A set of relation schemas
- Schema constraints
	1. Key constraint
	1. Null constraint
	1. Default
		- the default value if no value is specified


### From ER to Relational 
- Rules
	1. Entity Set -> Relation
		- Copy attributes and key
	1. Relationship -> Relation
		- Attributes: need to store the primary keys of the entities as well as the attributes of the relationship
		- Key: only need the primary keys from the entities that cannot be functionally dependent
	1. Combining Many-One Relationships
		- Combine the relation of a many-one relationship with the relation of the "many"-side entity set. 
			- copy attributes from the relationship relation to 'many' entity set
- Translating ER subclasses to relational approaches
	1. ER approach
		- i.e. uses the normal ER rules and stores subclass as a relation 
			- with only the additional attributes
			- Primary key is the same as the one for superclass; link to superclass via the key
	1. OO approach
		- stores the subclass as a relation with all its superclass' attributes + its own attributes
	1. Null value approach
		- stores both superclass and subclass in the same relation


## Object-based modelling
- Types
	1. OO Data model
	1. Object-relational data model
		- PostgreSQL


## NoSQL modelling
- Data models
	1. Key-value model
		- e.g. Redis
	1. Document model
		- Documents do not necessarily share the same schema
		- JSON: JavaScript Object Notation
			- Standard for serializing data object in human-readable format
		- e.g. MongoDB
	1. Graph model
		- relationship as the first class concept
		- e.g. Neo4j



# Week 3: Computing on Data

## Relational Algebra
- Algebra with relations as the operands. The output is also a relation
- Types
	1. Reduction: make a table (relation) smaller
		1. Selection (filter rows)
		1. Projection (filter columns)
			- duplicates are removed
	1. Combination: combine 2 tables
		1. Union
			- operands need to have the same schema
			- duplicates are removed
		1. Difference
			- operands need to have the same schema
		1. Cartesian product
			- attribute names should not clash for the operands. Can use renaming first to ensure.
	1. Renaming: change attribute names
	1. Derived
		1. Intersect
			- Derived from R1 - (R1 - R2)
		1. Theta Join
			- Cartesian product then filtered by the theta condition
		1. Natural Join
			- Combining all pairs of typles that agree on the 'common' attributes 
			- i.e. natural join == projection of theta join with the condition R1.{common} = R2.{common}
- NoSQL DB does not need to do joins because of
	1. sub-document embedding
	1. document referencing
	1. arrays

## MapReduce
- Steps
	1. Map
	1. Group
	1. Reduce



# Week 4: Designing Schemas
- Goodness of a schema depends on the domain characteristics
- Desired properties of a schema
	1. Minimize redundancy
	1. Avoid info loss
	1. Preserve dependency
	1. Ensure good query performance
- A good schema is called a normal form.
- To transform a bad schema into a good one is called normalizing the schema

## Functional Dependencies
- A form of constraint
- Notation: $$ A_1, ... A_m \to B_1, ... B_n $$
- Meaning: If any tuples agree on $$ 𝐴_1, ... , 𝐴_𝑚 $$ values, then they must also agree on
$$ 𝐵_1,...,𝐵_n $$.
	- i.e. the mapping is functional. (A many-to-one mapping is called a functional mathematically because the value of A can uniquely determine the value of B.)
- Reasoning
	- Armstrong's Axioms
		1. Reflexivity rule
		1. Augmentation rule
		1. Transitivity rule
	- Derived rules
		1. Splitting rule
		1. Combining rule
- Closure
	- All attributes that a set of attributes can functionaly determine
	- Denoted as $$ A^+ $$
- An FD by non-key attributes can cause redundancy.

## Keys
- Primary key functional determines all attributes of a relation
- Superkey: A set of attributes that contains a key



## Normal Forms
- First Normal Form
	- Each attribute contains only single atomic values
- Boyce-Codd Normal form
	- A relation R is in BCNF if and only if: Whenever there is a nontrivial FD for R, $$ A \to B $$, then A is a superkey for R.
		- A functional dependency for A determines B is trivial if it simply determines itself, when B is a subset of A.
	- Whenever a set of attributes of R determines another attribute, it should determine all attributes of R.
	- BCNF Decomposition
		- Decompose into a good DF set and a bad DF set
		- Decomposition is lossless
		- BCNF may not be dependency preserving

# MP 3
## MySQL create table commands
```
CREATE TABLE Faculty(
	id INT not null,
	name VARCHAR(255) not null,
	position VARCHAR(255),
	researchInterest VARCHAR(255),
	email VARCHAR(255),
	phone VARCHAR(255),
	photoUrl VARCHAR(255),
	affiliation_id VARCHAR(255),
	PRIMARY KEY(id));

CREATE TABLE Affiliation(
	id INT not null,
	name VARCHAR(255) not null,
	photoUrl VARCHAR(255),
	PRIMARY KEY(id));

CREATE TABLE Keyword(
	id INT not null,
	name VARCHAR(255) not null,
	PRIMARY KEY(id));

CREATE TABLE FacultyContains(
	faculty_id INT not null,
	keyword_id INT not null,
	score DOUBLE not null,
	PRIMARY KEY(faculty_id, keyword_id));

CREATE TABLE Publication(
	id INT not null,
	title VARCHAR(255) not null,
	venue VARCHAR(255),
	year INT not null,
	numCitations INT not null,
	PRIMARY KEY(id));

CREATE TABLE FacultyPublishes(
	faculty_id INT not null,
	publication_id INT not null,
	PRIMARY KEY(faculty_id, publication_id));

CREATE TABLE PublicationContains(
	publication_id INT not null,
	keyword_id INT not null,
	score DOUBLE not null,
	PRIMARY KEY(publication_id, keyword_id));


```

## Load Data
```
LOAD DATA LOCAL INFILE '~/Downloads/publication_contains.csv'
INTO TABLE PublicationContains 
FIELDS TERMINATED BY ','  
ENCLOSED BY '"'
LINES TERMINATED BY '\n' 
IGNORE 1 ROWS;
```

# Week 5: Querying relational databases
- Query language: A “computer language” to communicate with a database for asking questions on the information stored within.


## SQL
- Standard language for querying and manipulating data.
- Declarative language: only say what you want, and not how to get it.
- Benefits
	1. Easy to use
	1. Efficient
- Features
	1. Closure property - both input and output are relations (even a scalar value is a relation)
- Basic: SELECT ... FROM ... WHERE ...
	- In terms of relational algebra: selection & projection
- Null values
	- Common cases
		1. Missing value
		1. Inapplicable
	- When NULL is compared with any value, the truth value is UNKNOWN.
	- Test for NULL: x IS NULL, x IS NOT NULL
- Subquery
	- Can occur in SELECT / FROM / WHERE
	- Operators that deal with a relation
		1. IN
		1. EXISTS
		1. ANY
		1. ALL
- Set operators
	1. UNION
	1. INTERSECT
	1. EXCEPT
		- Difference
- Duplicates
	- Set vs Bag
	- Override bag semantics: DISTINCT
	- Override set semantics: ALL
- Aggregation
	- Operators
		1. SUM
		1. AVG
		1. MIN / MAX
		1. COUNT
	- Framework
		1. FROM-WHERE
		1. GROUP-BY + attribute (to segregate groups)
		1. Functions (aggregate groups)
		1. HAVING + condition (filter groups; applies to each group)
		1. SELECT
	- Null does not contribute to aggregation
- JOIN

# MP4
## 2.3
```
SELECT k.name, count(p.id)
FROM keyword as k
JOIN publication_keyword as pk ON k.id = pk.keyword_id
JOIN publication as p ON p.id = pk.publication_id
WHERE p.year >= 2012
GROUP BY k.name
ORDER BY count(p.id) DESC
LIMIT 10;
```

## 2.4
```
SELECT f.name
FROM faculty as f
JOIN faculty_publication as fp ON f.id = fp.faculty_id
JOIN (select * from publication WHERE publication.year = 2012 AND publication.num_citations > 10) as p ON p.id = fp.publication_id
GROUP BY f.name
HAVING count(fp.publication_id) > 10
ORDER BY trim(f.name);
```

## 2.5
```
SELECT u.name, count(DISTINCT f.id)
FROM keyword as k
JOIN faculty_keyword as fk ON k.id = fk.keyword_id
JOIN faculty as f ON f.id = fk.faculty_id
JOIN university as u ON u.id = f.university_id
WHERE k.name LIKE '%data%'
GROUP BY u.name
ORDER BY count(DISTINCT f.id) DESC
LIMIT 1;
```


## 2.6
```
SELECT f.name, sum(pk.score * p.num_citations) as 'KRC'
FROM keyword as k
JOIN publication_keyword as pk ON k.id = pk.keyword_id
JOIN publication as p ON p.id = pk.publication_id
JOIN faculty_publication as fp ON p.id = fp.publication_id
JOIN faculty as f ON f.id = fp.faculty_id
WHERE k.name = "machine learning"
GROUP BY f.name
ORDER BY sum(pk.score * p.num_citations) DESC
LIMIT 10;
```

## 2.7
```
SELECT p.title
FROM keyword as k
JOIN publication_keyword as pk ON k.id = pk.keyword_id
JOIN publication as p ON p.id = pk.publication_id
JOIN faculty_publication as fp ON p.id = fp.publication_id
WHERE k.name = "machine learning" AND p.year < 2016
GROUP BY p.id
ORDER BY count(fp.faculty_id) DESC, p.title
LIMIT 10;
```


## 2.8
```
SELECT u.name
FROM keyword as k
JOIN faculty_keyword as fk ON k.id = fk.keyword_id
JOIN faculty as f ON f.id = fk.faculty_id
JOIN university as u ON u.id = f.university_id
WHERE (k.name = 'machine learning' or k.name = 'computer vision' or k.name = 'natural language processing' or k.name = 'data mining' or k.name = 'information retrieval')
AND (u.name = 'University of Illinois at Urbana Champaign' or u.name = 'University of California--Berkeley' or u.name = 'Stanford University')
GROUP BY u.id
ORDER BY count(DISTINCT f.id) DESC
LIMIT 1;
```


# Week 6: NoSQL

## MongoDB
- Methods
	1. db.collection.aggregate()
	1. db.collection.count()
	1. db.collection.distinct()
	1. db.collection.mapReduce()

# Week 7: Neo4j
- Cypher
	- Query language for Neo4j


# MP5

## MongoDB
- `show databases`
- `use academicworld`
- `show collections`
- `db.faculty.findOne()`
- MongoDB
	1. `db.faculty.count({position:"Assistant Professor"})`
	1. `db.faculty.find({"affiliation.name": "University of illinois at Urbana Champaign"}, {"name": 1, "phone": 1, "email": 1, "_id": 0}).sort({ "name": 1 }).limit(10);`
	1.
	```
	db.publications.aggregate([
		{ $unwind: "$keywords" },
		{ $match: { "year": { $gt: 2011 } } },
		{
			$group: {
			_id: "$keywords.name",
			pub_cnt: { $sum: 1 }
			}
		},
		{ $sort: { "pub_cnt": -1 } },
		{ $limit: 10 }
	]);
	```
	1.
	```
	db.faculty.aggregate([
		{
			$match: {
				"affiliation.name": "University of illinois at Urbana Champaign"
			} 
		},
		{
			$project: {"_id": 0, "name": 1, "publications": 1}
		},
		{ $unwind: "$publications" },
		{
			$lookup: {
				"from": "publications",
				"localField": "publications",
				"foreignField": "id",
				"as": "pubDetails"
			}
		},
		{
			$project: {"name": 1, "pubDetails": { $arrayElemAt: ["$pubDetails", 0] } }
		},
		{
			$match: {
				"pubDetails.keywords": {
					$elemMatch: {
						"name": "data mining"
					} 
				} 
			}
		},
		{ $unwind: "$pubDetails.keywords" },
		{ $match: { "pubDetails.keywords.name": "data mining" } },
		{
			$group: {
				_id: "$name",
				KRC: { 
					$sum: {
						"$multiply": [
							"$pubDetails.numCitations",
							"$pubDetails.keywords.score"
						]
					}
				}
			}
		},
		{ $sort: { "KRC": -1 } },
		{ $limit: 10 }
	]);
	```
- Neo4j
	1. 
	```
	MATCH (n)
	WHERE n.position = "Assistant Professor"
	RETURN COUNT(n)
	```
	1. 
	```
	MATCH (f:FACULTY)-[r:INTERESTED_IN]->(k:KEYWORD) 
	RETURN k.name, COUNT(f)
	ORDER BY COUNT(f) DESC
	LIMIT 10;
	```
	1.
	```
	MATCH (f1:FACULTY)-[r1:PUBLISH]->(p:PUBLICATION)<-[r2:PUBLISH]-(f2:FACULTY),
		(f1:FACULTY)-[r3:AFFILIATION_WITH]->(i1:INSTITUTE),
		(f2:FACULTY)-[r4:AFFILIATION_WITH]->(i2:INSTITUTE)
	WHERE i1.name = "University of illinois at Urbana Champaign"
		AND i2.name <> "University of illinois at Urbana Champaign"
	RETURN i2.name, COUNT(distinct f1)
	ORDER BY COUNT(distinct f1) DESC
	LIMIT 10;
	```
	1.
	```
	MATCH (f:FACULTY)-[r1:PUBLISH]->(p:PUBLICATION)-[lb:LABEL_BY]->(k:KEYWORD)
	WHERE k.name = "data mining"
	RETURN f.name, SUM(lb.score * p.numCitations) AS accumulated_citation
	ORDER BY accumulated_citation DESC
	LIMIT 1;
	```
	1.
	```
	MATCH
	  (cz:FACULTY {name: 'Craig Zilles'})-[r1:AFFILIATION_WITH]->(i1:INSTITUTE),
	  (f:FACULTY)-[r2:AFFILIATION_WITH]->(i2:INSTITUTE),
	  p = shortestPath((cz)-[:INTERESTED_IN*]-(f))
	WHERE i1.name = "University of illinois at Urbana Champaign"
		AND i2.name = "Carnegie Mellon University"
	RETURN p
	LIMIT 5
	```

# Week 8: Manipulating DBs
- Need to keep DB consistent and up to date.
- DB management process
	- Real world -> Data modelling -> Manipulating DB -> Querying DB
- DB basic functions: CRUD
- DB data elements: Database, table, schema, tuple, attribute.

## Organzing DBs
- a database consisting of multiple schemas, which consist of multiple tables.
	- Schema here refers to a scope / space of data instead of a structure of a table / database
	- Schema contains data types, functions, and operators
- A constraint is a relationship among data elements that the DBMS is required to enforce.
	- Types
		1. key constraints
			- PK, FK
		1. attribute-based constraints
			- specific value of an attribute
		1. tuple-based
			- specific entry in the table
		1. database-based
			- relationship among elements in a DB

## Modifying DBs
- Insert
- Delete
- Update
	- Example: ```
	UPDATE <name>
	SET <list of attribute assignments> 
	WHERE <condition>;
	```
	- "Deferred Updates"
		1. Evaluate WHERE expression and identify every tuple that needs to be updated.
		1. Update the tuples identified.


## Indexes
- An index is a “map” of a table, telling DBMS where different values are stored.
	- Need indexes for the attributes that are frequently used in queries s.t. DBMS can process queries quickly
- Creating index 
	- DBMS may create an index automatically for important attributes
	- Users can tell DBMS what attributes to create index for, since users anticipate what queries will be asked
- Commands
	- `CREATE INDEX <name> on <table> (<attributes>);`
	- `SHOW INDEX FROM <table>;`
	- `ALTER TABLE <table> DROP INDEX <name>;`
- Every time a row is updated, all the indexes on the column(s) affected need to be modified as well. As we have a higher number of indexes, the load at the server rises to keep all the schema objects up to date and this tends to slow things down.

## Views
- A virtual table




# Week 9: Regulating Databases
## Key Constraints
- PK
- FK
	- Referential integrity
	- Constraint violations when S (source) references T (target)
		1. Source change
			- Mitigated by rejecting
		1. Target change
			- Mitigated by
				1. Reject
				1. Cascade
				1. Set NULL
	- FOREIGN KEY declaration provides more thorough referential integrity guarantee than CHECK() does.
		- CHECK() is activated only when the source table itself is modified (via INSERT or UPDATE).
- Unique keys
	- PK vs unique keys
		1. PK can only have one set of attributes
		1. PK cannot be null

## Checks and Assertions
- Checks
	1. Attribute-based: Constrain the value of a particular attribute in a tuple.
		- checked only when the attribute is inserted or updated.
		- e.g. price float CHECK (price > 1.0)
	1. Tuple-based: Constrain the value of multiple attributes in a tuple.
		- only when a tuple is inserted or updated
- Assertions
	1. Database-based
		- format: `CREATE ASSERTION <name> CHECK ( <condition> );`
			- Condition may refer to any relation or attribute in the database schema.
		- declared as part of the database schema



## Triggers
- Checks and Assertions limitations
	1. timing
	1. flexibility (on what to check)
	1. handling
- ECA (Event-Condition-Action)
	- Event = Preposition (after, before, instead of) + event (insert, delete, update)
		- A trigger for a view should be activated using the `INSTEAD OF` event timing keyword
	- Condition
		- `REFERENCING [NEW / OLD][ROW / TABLE] AS <name>`: the data elements
				- `OLD TABLE`: the entire set of deleted rows
		- `FOR EACH ROW / STATEMENT`
		- `WHEN`: define the constraint violation


# Week 10 - 11: Accessing and indexing data
- Query processor steps
	1. Parsing
	1. Optimization (generate a query plan)
	1. Processing
	1. Data Indexing
	1. Data Accessing
		- Buffer mangager deals with "buffering" data in memory for processing. 
		- storage manager manages I/O to storage.
			- row storage vs column storage
- Accessing data on disk
	1. Random Access
		- Cost = seek time + rotational latency + transfer time
	1. Sequential Access
		- Cost = transfer time

## Indexing
- An index provides a "map" for locating data
	- definition: A data structure that provides a mapping from a search key value to the locations, or pointers, of data that matches the key.
- Indexing: the action of building an index
	- objective is to store a collection of pointers; then use keys to label these pointers to follow.
- DBMS auto manages or lets users manage indexes.
- Dense index: having a pointer to every key value.
- How to store pointers?
	1. For pointer to a key
		- store n pointers and n keys in a disk block
	1. For pointer to a set of keys
		- An index block contains n keys and n+1 pointers
		- A tree structure
		- label each pointer by a range [left, right)
		- Fanout: the number of pointers F = n + 1
		- e.g. ISAM, B+ tree
- Index classification
	1. Clustered vs unclustered
	1. Dense vs sparse
	1. Primary vs secondary
	1. Structure (tree vs hash table)
		- Hash Tables are faster on average than B+ Trees for ‘equality’ based lookups (i.e., checking if an attribute is equal to a certain value)
		- B+ tree is better in range scan or lookup a prefix of a key



### ISAM (Indexed Sequential Access Method)
- Combining above 2 ways to store pointers
- A tree structure
- Lookup - tree traversal
- Cost = height of tree i.e. $$ \log_F(N) $$
	- ISAM trees are normally fully packed
- Cons - dealing with dynamic data
	1. Adding new data can result overflow blocks; increasing tree height & lookup time
		- overflow blocks would not be sorted automatically
	1. Disk blocks might have empty spaces
	1. For static hash tables, if we want to resize the table by adding more buckets, we would need to create a new hash function.


### B-tree (also B+)
- Addresses the cons from ISAM on balance of tree and disk block utilization
- New parameter d for degree (aka order)
	- determines the size of the node
	- common values: 1, 1.5, 2, 2.5
	- Max num of keys: = 2d
	- Min num of keys: n = d
		- Leaf node: n = cell(d)
		- Internal node: n = floor(d) 
- Pointer labeling: same as ISAM
- In practice
	- Typical degree: 100
	- Typical fill-factor: 67%
	- Average fanout = 133
- Operations
	1. lookup
		- point queries: same as ISAM
		- range queries: perform a point query then traverse using 'next-leaf' pointers
	1. update
		- Principle: When a node becomes too full or too empty, perform "local" changes to maintain the tree in a good shape.
			- A "local" change should involve only siblings: Split, re-distributing pointers, merge.
			- Goal
				1. Every node satisfies the node capacity requirement
				1. Every node has some n + 1 pointers labeled by n keys.
			- The changes will propagate up the tree recursively if needed
			- Adjust keys after local changes
				- key = min(right tree)
			- can result in more than one tree structure
		- Insertion

### Hash Indexing
- Hash table: Mapping key -> bucket b by hashing
- Hash function: Compute key to hash value mapping
- Bucket: $$ n = 2^k $$ as addressed by the hash function
- Static hashing
	- Operations
		1. query
		1. insertion
	- Like ISAM, static hashing is good for static data due to overflow blocks
	- Static Hash Tables may not always have faster insert than Dynamic Hash Tables.
- Dynamic hashing
	- Principle of hash-to-bucket organization
		- Organize the hash code space, to allocate codes to buckets dynamically, depending on how many buckets are needed.
		- Use only i most/least significant bits, i <= k
		- Partition into at most $$2^i$$ buckets depending on the needs
	- When a dynamic hash table is being reorganized, we only need to touch the particular bucket that is being split/extended.
	- Extensible hash table (MSB)
		- can merge more than 2 neighbour buckets
		- nub shows the number of bits used i.e. j
		- Operations
			1. lookup (search MSB)
			1. insertion
				- when bucket is full
					1. split bucket
					1. double directory
		- extensions on the table can be costly and take a long time.
	- Linear hash table (LSB)
		- can merge at most 2 far away buckets
		- can have overflow blocks
			- number of overflow blocks controlled by average capacity parameter u (requires $$ \frac{r}{n} < u $$) where r is num of keys and n is num of buckets
		- no directory
		- Operations
			1. lookup
				- can perform flipping
			1. insertion
				1. if full, can use overflow block
				1. if cannot use overflow, add one more bucket and flipping
		- Extensible Hash Tables require more memory because there are no overflow blocks. This means that the entire directory needs to fit into the main memory


# Project
## Dash
- dcc (Dash Core Components)
	- Gives you access to many interactive components, including dropdowns, checklists, and sliders.
	- Functionalities
		1. Graph component called `dcc.Graph`, which is used to render interactive graphs.
- Controls and Callbacks

## DB Techniques
- Indexes
	- `CREATE INDEX IF NOT EXISTS keyword_name_index on keyword (name);`
	- `SHOW INDEX FROM keyword;`
	- `SHOW INDEX FROM keyword WHERE KEY_NAME = 'keyword_name_index';`
	- `DROP INDEX keyword_name_index ON keyword;`
- View
	- Create a view on university + faculty
	- ```
	CREATE VIEW faculty_university AS
SELECT faculty.id, university.name
FROM faculty
JOIN university ON university.id = faculty.university_id;
	```
	- `drop view faculty_university;`
- Constraints
	- Key constraints via new table
	- ```
CREATE TABLE IF NOT EXISTS favorite_keyword(
id INT not null AUTO_INCREMENT,
keyword_id INT not null,
PRIMARY KEY(id),
FOREIGN KEY(keyword_id) REFERENCES keyword(id));
	```
	- `INSERT INTO favorite_keyword (keyword_id) VALUES (49323);`
	- ```
	drop table favorite_keyword;
	```