# Week 1

# Week 2 - Classification

## NN
- stores training examples instead of constructing a general model of training data
- computes the distance from the new example and all the examples in the training set
- what can be done when data features come at different scales?
	- recenter the features around 0 and rescale them to have a variance of 1
- Issues
	1. too sensitive to the distribution of the data
		- if one of the items in the dataset is missing, the resulting label may change
		- can resolve by using KNN
	1. classifying images 
		- it would be hard to collect a compact training set with enough representative images to achieve good performance
	1. sensitive to potentially irrelevant features

### KNN
- a method that reports the class of the majority of the k-closest elements in the training data to the new example
 

## Naive Bayes
- assumes that the different features are independent from each other conditioned in the class
- constructs a general model of the training data
- computes the probability distribution P(y) of all classes y from the training examples
- When classifying a new test example, the Naive Bayes algorithm reports the class label with maximum probability given the features of the test example
- More accurate when missing entries are kept as 0s than when they are ignored in the computation

## Cross validation
- An iterative process that splits data into a training set for training the model and a testing set to compute error in each iteration
- Allows us to compare the performance of different options of models to represent the data


# Week 3 - Random Forests, SVM

## Random Forests
- Based on decision trees
- make a decision based on the classification of all the trees
- Information Gain for a dataset that results in two subsets
	- high when each of the resulting subsets have members of only one class
- Stopping conditions for a branch of the recursive algorithm to build decision trees include
	1. when all the elements in a branch correspond to the same class
	1. when reaching a maximum depth

## SVM
- A binary classifier 
	- i.e. whose classes are identified through either positive or negative values
- Trained with a cost function known as **hinge loss**
- Features
	1. learns a linear decision boundary $$ a^Tx + b $$
		- A hyperplane that separates positive data from negative data
		- Boundary line: $$ x = -\frac{b}{a} $$
	1. easy to train
	1. fast
- Limitations
	1. Features need to be in real values
	1. No missing features
	1. Classes separable through a linear function
	1. Need enough features
- Cost function
	- takes prediction value and the corresponding label
	- ensures that each example in a linearly separable training set is on the right side of the decision boundary
	- C = training error cost + regularization constant * penality
		- penalizes errors on query examples
		- regularization constant 
			- found through cross-validation on several options to select the one with best accuracy

### Stochastic Gradient Descent
- A method for training SVM
- Prerequisite
	1. Rescale data s.t. each feature has unit variance
- Goal
	- Find a and b to minimize cost function
- Process
	- Iterative process
	- searches for the optimal values for the Hinge Loss by following its gradient
- estimates the mean gradient from batches samples much smaller than the dataset

### Regularization Constant
- Can try different values

# Week 4 Regression

## Linear Regression
- Residual = actual y - predicted y (the difference between a measurement and a prediction)
	- Least square residuals
		- the difference between the actual value of the dependent variable and the prediction generated by the regression
- Goal: minimize residual, which is also the mean squared error

### Performance
- Outliers
	- correspond to items in the dataset that are away from most of the other items
	- have a higher Cook's distance than most of the other points
	- have higher leverage than most other items in the dataset
	- may come from errors when collecting the data, but may also come from infrequent events that are indeed part of the modeled process
- Regularization
	- requires a regularization coefficient that is determined through cross-validation
	- penalizes large values of the coefficients

### Transformation
- Can be applied to polynomial functions if the explanatory variables incorporate the power terms


# Week 5
- In regression models, error due to noise in modeling can be attributed to phenomena inherent to the process being modeled that are impossible to be explained by the input features


## Information Criteria
- Types
	1. Akaike info criteria (AIC)
	1. Bayes info critera (BIC)
- Information Criteria metrics have lower cost with both lower number of coefficients and lower error in predictions

## M-estimators
- Cost function
	- gives large weight to errors at points with low residuals
	- gives low weight to errors at points with high residuals

## Models
- Types
	1. Logistic Regression models
	1. Lasso Regression
		- may give a value of zero to coefficients of explanatory variables
		- may remove correlated explanatory variables
	1. Elastic Net
- models that generalize beyond traditional regression assumptions
	1. Logistic Regression models
	1. Regression models for probabilities of counts

## Huber loss
- grows quadratically for small values of $$u \in [-\sigma, \sigma] $$
- grows linearly for large values of $$ u \in \sigma $$



# Week 6 High dimensional data
- Difficulties
	1. distance between data items in higher dimensions are bigger as dimensionality increases
	1. the number of histogram boxes grow significantly with the number of dimensions
- Blob
	- is a subset of nearby points that are farther from other points
	- can be transformed through affine transformations that preserve collinearity and ratios of distances between points

## Covariance Matrix
- The covariance matrix for a dataset with N items and d features
	- is a square matrix with d rows and d columns
	- each entry hold the covariance between feature i and feature j
	- has non-negative eigenvalues
	- is symmetric


## Principal Component Analysis 
- Functionality
	1. can construct a representation that uses less features
	1. the subset of features with the largest eigenvalues in the covariance matrix of the dataset are selected
	1. the error of the model that results of the selection of s features with respect to the original model can be computed as a function of the eigenvalues that correspond to the dâˆ’s not selected features

# Week 7
## NIPALS
- is a good choice to compute a few principal components when X is large
- efficiently computes the largest principal component of X


# Week 9
## EM
- data items are clustered based on a mixture of probability distributions
- each cluster is modeled by a probability distribution
- customization to the type of probability distribution being modeled is needed
- E step
	- determines the weights that assign points to each specific distribution or cluster


# Week 11

## Markov Chain Models
- use a set of states for representation purposes
	- use probabilistic transitions between states
- Features
	- may model a number of chains each of them being a biased random walk on the graph that models the states and their transition probabilities
	- can be simulated to find the probability of arriving at some state after a number of steps
	- can have a probability distribution for the initial states
	- require that the sum of probabilities of outgoing edges for a state sum to one
	- may have a stationary distribution of states that stabilize and is independent of the initial state for irreducible chains


## HMM (Hidden Markov Models)
- have an emission distribution that models the probability of an observation in a given state
- associate a set of observations linked to the hidden states
- use a set of Hidden States and Transition Probabilities among them as in Markov Chains
- can be used to infer the Markov chain that most likely produced a sequence of observations


## Viterbi algorithm
- uses Maximum a Posteriori Inference to estimate the Markov chain that maximizes the posterior probability of the states conditioned on the observations and the other components of the Hidden Markov Model
- uses a cost function that only considers the logs of pairs of probabilities, one that accounts for the transition probabilities, and the other that accounts for the emission probabilities
- is an algorithm that follows a dynamic programming approach


# Week 12
## Mean Field Inference
- uses a simpler probability distribution than the one in the original model
- finds an approximate solution to maximize the posterior probability of the hidden values in the model conditioned on the observations
- reduces the search space by taking advantage of the neighborhood structure of the hidden values
- aims to maximize the intensity in the connections between adjacent nodes

## Boltzmann Machine
- has only binary hidden values
- has a probability distribution with a very large number of possible hidden values which are computationally too expensive to compute in many practical applications
- is a particular case of Discrete Markov Random Fields


## Discrete Markov Random Field
- extends the Boltzmann Machine to allow for discrete values for the nodes


## KL divergence
- is used to develop a tractable distribution to apply Variational Inference
- allows to maximize the likelihood of a dataset under one distribution by minimizing the KL divergence between the two distributions
- measures the dissimilarity between two distributions
- between two distributions Q and P is minimized by minimizing the variational free energy under probability Q



# Week 13: Neural Networks
## Neural Network
- a method for classification
- slow to train, even more when it has many layers with many units.
- suitable to identify appropriate features in the data
- performs better when trained with datasets of the "appropriate" size, which is usually large.
- finds piece-wise linear boundaries that separate data items that belong to different classes
- Fully connected layer
	- connects each input to every unit in the layer
- Linear unit
- ReLU unit
	- In combination with the linear weight of the inputs and the bias, splits the inputs into points that are at one side of a boundary or at the other side.
	- Activates the unit output so that it transfers the input to the output when the input is positive, otherwise the output is 0

### Softmax layer
- A generalization of the sigmoid function to N dimensions, where N is the number of classes, and outputs in the layer
- Balances the probabilities among all the outputs in the unit so they sum 1
- Is connected the output of nonlinear units to provide outputs with estimate of the probability of the input to belong to that class

### Cost function
- has a loss component
- is used to find the parameters that minimizes it by exploring its gradient through Stochastic Gradient Descent
- has a regularization component whose parameter Î» is used to reduce weights and is found through cross-validation


### Backpropagation
- searches for the parameters that reduce the loss at the output of a unit (every unit) through stochastic gradient descent
- has a forward pass to propagate inputs to the output
- has a backward pass where the gradient of the loss is propagated from the output towards the input


## DNN
- Different layers may have different types of units
- There is a sequence, or stack, of layers
- Negative effects due to redundant units can be prevented through dropout, which allows the network to be robust to redundant units that may be ignored at some layer
- There usually is one input layer, several hidden layers and one output layer with the softmax function



# Week 14: CNN

## Convolution
- operates using an input image and a kernel which are both arrays with the same number of dimensions
- features
	1. commutative operation
	1. results in a large positive value when the pixels in the image match the corresponding pixels in the kernel
	1. can be computed for RGB images through a 3D version where each of the color intensities is encoded as a channel

### Kernel
- an array that encodes a pattern to be identified in the input image
- lays partially outside of the image when the center is within k pixels of any edge, when the kernel has 2 * k + 1 rows or columns and no padding was used
- can be learned in a NN convolutional layer

### Stride
- When convolutions are taken in strides of length s > 1
	- s is the number of steps taken in each dimension to move the kernel to compute the next convolution value
	- the size of the output in each dimension is about the size of the input in each dimension divided by s
	
### Feature Map
- Holds the response to a collection of L patterns, or features, in the image and is computed through convolution
- encodes pattern activation in locations in the image
- can be computed using kernels

## Convolutional layers
- implements the convolution operation through units whose weights are kernel values
- can be setup with parameters that include the number of kernels, the number of channels, the stride length and the padding strategy used
- shares the kernel weights among the convolutional units associated to the same slice on the output feature map
- outputs a feature map that encodes in each slice the activation patterns encoded in the kernels whose parameters are learned
- have a receptive field with respect to the input to the network whose size depends on the settings on previous layers on stride lengths and kernel sizes
- trained to learn kernel weights and biases for its units