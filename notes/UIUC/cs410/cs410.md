# Week 1

## Intro
- Motivation: harnessing Big text data
	- Text retrieval：which helps identify the most relevant text data to a particular problem from a large collection of text documents, thus avoiding processing a large number of non-relevant documents	
		- converts big text data into smaller relevant data
	- Text mining, which helps users further analyze and digest the found relevant text data and extract actionable knowledge for finishing a task	
		- converts smaller data into knowledge

## Natural Language Content Analysis
- Computers have to understand natural languages to some extent, in order to make use of the data for further analysis
- Translation (from English to French) relies more on NLP than comparison in the same langauge

### Natural Language Processing
- Progression
	1. Lexical analysis (speech/POS tagging): figure out which words are nouns, verbs, adjectives, etc.
	1. Parsing: determine the structure of a sentence
	1. Syntactic analysis
	1. Semantic analysis
	1. (Optional) Inference
	1. Pragmatic analysis (speech act analysis): why someone say this sentence
- Types of challenges
	1. Word-level ambiguity
		- Same word can have different meanings / different forms (verb or noun)
	1. Syntactic ambiguity
		- Refers to multiple possible structures of the same sentence or phrase
		- e.g. Prepostional phrase attachment ambiguity
	1. Anaphora resolution
		- e.g. 'A ask B to do something for himself.' (himself = A or B)
	1. Presupposition

### State of art
- Can do lexical analysis and partial syntactic analysis pretty well, to some extent in semantics, and fairly bad in inference / speech act analysis
- Robust & general NLP tends to be shallow
- Deep understanding does not scale up

### NLP for text retrieval

## Text Access
- Push 
- Pull
	- user takes initiative
	- typically request ad hoc (temporary) information
	- Query vs Browsing

## TR (Text Retrieval) Problem
- TR (Text Retrieval)
	- A task where the system would respond to a user's query with relevant documents.
	- Supporting pull mode of info access
	- e.g. search in google
	- TR is a subset of IR (information retrieval), which also includes retrieving non-textual information including video, audio, etc
- TR vs Database query
	1. Information
		1. Unstructured vs structured
		1. Ambiguous vs well-defined semantics
	1. Query
		1. Ambiguous vs well-defined semantics
	1. Answers
		1. No 'right' answers vs matched recorded
	1. TR is an empirically defined problem
		- Effectiveness is evaluated by users

### Formal Formulation
1. Vocab: $$ V = {w_1, w_2, ... }$$ of language
1. Query: $$ q = q_1, q_2, ... q_m $$ where $$ q_i \in V $$ (i.e. a sequence of words)
1. Document: $$ d_i = d_{i1}, ..., d_{im_j}$$ where $$d_{ij} \in V $$ (i.e. each document is a sequence of words, but typically is longer than a query)

### Solving TR Problems
1. Document selection
1. Document ranking
	- $$ f(d, q) $$ where d is one of the docs, and q is the query. f returns a score which will be evaluated against a threshold
	- generally preferred becuase
		1. help users prioritize examination of search results
			- not all relevant docs are equally relevant
		1. Bypass the difficulty in determining absolute relevance
	- challenge: define an effective ranking function


## TR Methods
- Design of ranking fucntion requires a computational definition of relevance

### Retrieval Models
1. Similarity-based models
1. Probabilistic models
	- $$f(d,q) = p(R=1|d,q) $$ where $$ R \in {0, 1} $$ is a random variable representing relevance
1. Probabilistic inference model
	- $$ f(q,d) = p(d \to q) $$
1. Axiomatic model
	- $$f(d,q)$$ must satisfy a set of constraints

### Common stats in retrieval models
- Term frequency
	- how many times does a word occur in a doc
	- also denoted as $$ c(w, d) $$
- Document length
	- How long is d
- Document frequency
	- how many docs contain the word
	- also denoted as $$ df(w) $$

### Retrieval Model Examples
1. Bag of words
	- $$ f(q, d) = \sum_{i = 1 ... n} g(q_i, d) $$ where g is the weight of each words, which represents how well the doc matches each of the query words
		- Thus the total score depends on the score of each word
	- Certain information about text is lost
		1. word ordering
		1. phrases formed by multiple words
1. Pivoted length normalization
1. BM25 (most popular)
1. Query likelihood
1. PL2


## VSM (Vector Space Model)
- A similarity based model: $$ f(q,d) = similarity(q,d) $$
- Steps
	1. assume that all our documents and the query will be placed in this vector space
	1. measure the similarity between query and every other documents

### VSM as a framework
- Components
	1. Term is used to represent a basic concept; it can be a word / phrase
		- Each term defines one dimension
	1. N terms define an N-dimensional space
	1. Query vector: $$ q=(x_1, ... x_N) $$, where x is query term weight
	1. Doc vector: $$ d=(y_1, ... y_N) $$
- It's a framework because the following is left to be determined by the user
	1. which basic concept to choose
	1. how to assign term weights
	1. how to define similarity measure 


## VSM Simple Instantiation
- BOW (bag of words) as basic concept + bit vector (1 if word is present) as term weight + dot product as similarity measure
- Cosine similarity is a better similarity measure than L2 distance



# Week 2

## VSM Improved Instantiation
- Bag of words with phrases is in general sufficient
- Dot product works well as well for similarity measure
- Thus the only improvement area would be term weight
- Steps
	1. Term frequency as term weight to give more credit for multiple occurrences
	1. Adding IDF (inverse document frequency) on top of term frequency to penalize words with high global count
- i.e. $$ y_i = c(W_i, d) * IDF(W_i) $$, where $$ IDF(W) = \log(\frac{M+1}{k}) $$ where M is number of docs and k is the number of docs where W is presented
- The term weight can be referred as TF-IDF
- Stop words
	- Very common words including 'the', 'is', 'which'

## TF Transformation
- Problem of TF-IDF: documents with high single word count would have a very high score
- Solution: TF transformation
	- Best Method: BM25
		- $$ TF(w,d) = \frac{(k+1)x}{x+k} $$ where TF(w,d) is the new term frequency weight, k is a parameter and also k+1 is the upper bound, and x is the original TF i.e. c(w,d) 





## Doc Length Normalization
- Besides TF-IDF and TF transformation, we also want to penalize a long doc with a doc length normalizer 
- Pivoted Length Normalization
	- Average doc length as pivot
	- normalizer = $$ 1 - b + b\frac{|d|}{avdl} $$ where b is a constant $$\in [0,1]$$ and avdl is the avg doc length 
	- When b increases from 0 to 1, the amount of reward / penalization increases


## System Implementation of TR
1. Tokenizer
	- Tokenization
		1. Normalization: words with similar meanings should be mapped to the same indexing term
		1. Stemming: mapping all inflectional forms of words to the same root form
	- Benefits
		1. Reduces the number of terms
		1. Improves performance by mapping words into the same term
		1. Extracts words as lexical units from strings of text
1. Indexer
1. Scorer
1. Feedback


## Sys Implementation - Inverted Index
- Inverted index (aka postings list, postings file, or inverted file) is a database index storing a mapping from content, such as words or numbers, to its locations in a table
- Challenge: build a huge index with limited memory
- Steps
	1. map terms & docs to integers
	1. sort by doc id by loading one doc at a time to memory
	1. sort by term id within each doc
	1. merge sort by term id across all docs 

### Inverted Index Compression
1. TF Compression
	- small number for frequent words
1. Doc ID compression
	- d-gap - store differences
		- decoding: get x1 to get first doc, then decode by adding the differences
1. Integer 
	1. Binary: equal length coding (same as binary number)
	1. Unary: x-1 one bits followed by 0
		- e.g. 5 - 11110
	1. gamma-code
		- unary code for 1 + floor(logx) followed by uniform code for $$ x-2^{floor(logx)} $$ in floor(logx) bits
			- Encoding example: 5 -> floor(log5) = 2. Prefix = unary(3) = 110. Suffix = 1 in 2 bits. Thus 5 = 11001
			- Decoding: 11001 - split into 110 and 01, then we can get $$ 2^(3-1) + 1  = 5 $$
	1. delta-code
		- same as gamma, but the prefix is replaced with gamma code
			- Encoding example: 5 -> floor(log5) = 2. Prefix = gamma(3) = 101. Suffix = 1 in 2 bits. Thus 5 = 10101

## Sys Implementation - Fast Search
- $$ f(q,d) = f_a(h(g(t_1, d, q), ..., g(t_k, d, q)), f_d(d), f_q(q)) $$
	- $$f_a $$ is aggregator function
	- $$g$$ gives the weight of a matched query term $$t_i$$ in doc d
	- $$h$$ function aggregates all the weights
	- $$f_d(d), f_q(q)$$ are query level and doc level factors, which are pre-computed




# Week 3: Evaluation of TR System
- Precision
	- Ratio of relevant docs in result docs i.e. retrieved relevant docs / retrieved all
- Recall
	- Ratio of returned relevant docs of all relevant docs
- Ideal results: precision = recall = 1
	- In reality, high recall typically has low precision
- F measure
	- Combine precision and recall
	- Parameter weights between precision and recall
		- high value parameter: weighs more on recall
	- $$ F_1 = \frac{2PR}{P + R}$$

## PR Curve, AP
- AP 
	- Computing the area under the PR curve and then divide by the total number of relevant docs
	- A standard measure for comparing ranking methdods because
		1. combines precision and recall
		1. sensitive to the rank of every relevant doc

## MAP, Reciprocal rank
- MAP: arithmetic mean of AP over a set of queries
	- arithmetic mean could have bias since the sum is dominated by large values
- gMAP: geometric mean of AP over a set of queries
	- gMAP is affected more by low values
	- More useful for improving search engines with difficult queries

### Reciprocal Rank
- Only one relevant doc in the collection
	- AP = Reciprocal rank = 1/r, where r is the rank position of the single relevant doc

## Multi-level relevance judgments - nDCG
- AP, MAP, gMAP don’t work as they only work for binary judgments; instead, nDCG is used
- DCG
	- Discounted cumulative gain
	- Discounted means penalizing low ranked docs to give more weights to high ranked docs
- nDCG
	- Normalized means to use the ideal ranking to compute a theoretic upper bound and then normalize the actual gain value with the bound
	- i.e. $$ nDCG = \frac{DCG}{Ideal DCG} $$
	- Range: 0 - 1
	- Can compare 2 systems performed on a set of queries


## Practical Issues
- Testing challenges
	1. Query: need many representative queries
	1. Judgments - completeness vs. minimal human work
	1. Measures: capture the perceived utility of users
	1. Docs: need many representative docs
- Statistical Significance Test
- Pooling
	- avoids judging all docs




# Week 4: Probabilistic Retrieval Model
- Models: $$ f(d,q) = p(R=1|d,q), R \in {0,1} $$
	- R is a random variable representing relevance
	- $$ p(R=1|d,q) $$: given the query q and returned doc d, the probability that d is relavant
- Model types 
	1. Classic probabilistic model: BM25
	1. Language Model: query likelihood
		- $$ f(d,q) \approx p(q|d, R=1) $$
		- $$ p(q|d, R=1) $$: given the relavant doc d, how likely would the user enters query q to retrieve d.
	1. Divergence-from-randomness model: PL2

## Statistical Language Model
- A probability distribution over word sequences
- Stat LM features
	1. Context dependent
	1. A generative model - can sample words from the LM to form a doc
- LM benefits
	1. Quantify the uncertainties in natural language
	1. Distinguish between words with very similar sounds
- Use cases
	1. Representing topics
	1. Discovering word associations
	1. speech recognition
	1. text categorization
	1. information retrieval

### Unigram LM
- Simplest language model
- Features
	1. Ignore context
	1. Generating each word independently - easy to compute conditional probability
		- i.e. $$ p(w_1,w_2,w_3) = p(w_1)*p(w_2)*p(w_3) $$ 
	1. Order does not matter
- Maximum Likelihood Estimator
	- $$ p(w|\theta) = p(w|d) = \frac{c(w,d)}{|d|} $$ 


## Unigram Query Likelihood
- $$ p(q|d) = p(w_1,w_2,w_3,...|d) = p(w_1|d)*p(w_2|d)*p(w_3|d)... = \frac{c(w_1,d)}{|d|} * \frac{c(w_2, d)}{|d|} $$ (due to independence)
- Use estimated document language model $$p(w|d)$$ (to prevent 0 probability) and log (to avoid having a lot of small probabilities)
	- $$ f(q,d) = log(p(q|d)) = \sum^n_{i=1}log(p(w_i|d)) = \sum_{w \in V}c(w,q) * log(p(w|d)) $$
- How to estimate $$p(w|d)$$?
	- Smoothing

## Smoothing
- Ensures that p(w|d) > 0 even if c(w,d) = 0
- Challenge: how to assign probability to an unseen word
	- Solution: let the proability to be propotional to a reference LM e.g. collection LM
- Rewriting the query likelihood formula with smoothing
	- $$ f(q,d) = log(p(q|d)) = \sum_{w \in V}c(w,q) * log(p(w|d)) = \sum_{w \in V, c(w,d)>0}c(w,q)log(p_{Seen}(w|d)) + \sum_{w \in V}c(w,q)log(\alpha * p(w|C)) - \sum_{w \in V, c(w,d)>0}c(w,q)log(\alpha * p(w|C)) = \sum_{w \in V, c(w,d)>0}c(w,q)log(\frac{p_{Seen}(w|d)}{\alpha * p(w|C)}) + |q|log(\alpha) + \sum_{w \in V}log(p(w|C)) $$ where $$\alpha$$ is a parameter controlling the degree of collection LM probability
	- i.e. Sum of ML estimator for matched query words in d + collection LM probability for all query words - collection LM probability for the matched query words

## Specific smoothing Methods (i.e. to determine $$p_{Seen}(w|d)$$ and coefficient)
1. Linear Interpolation (Jelinek-Mercer) Smoothing
	- $$ p(w|d) = (1 - \lambda)\frac{c(w,d)}{|d|} + \lambda * p(w|C) $$ where $$\lambda$$ is between 0 and 1
	- does not incorporate doc length normalization
1. Dirichlet Prior (Bayesian) Smoothing
	- $$ p(w|d) = \frac{|d|}{|d| + \mu}\frac{c(w,d)}{|d|} + \frac{\mu}{|d| + \mu}p(w|c) $$
	- has a dynamic coefficient (not like the above $$\lambda$$) which depends on doc length
	- When u decreases, p is closer to the ML estimate derived from the doc i.e. $$ \frac{c(w,d)}{|d|} $$ 
	- incoporates doc length normalization




# Week 5

## Feedback in TR
- Types
	1. Relevance feedback
		- Explicit relevance judgements by users
	1. Pseudo / Blind / Auto Feedback
		- e.g. top-k initial results are simply assumed to be relevant
		- No user activity is required
	1.  Implicit feedback
		- User-clicked docs are assumed to be relevant


## Feedback in VSM
- Method: Query modification
	1. Adding new terms (query expansion)
	1. Adjusting weights of old terms

### Rocchio Feedback
- Steps 
	1. Projecting all queries and docs in a 2D space
	1. Try to move query to the centroid of relevant docs
- Formula
	- new query = original query + centroid of rel docs - centroid of non-rel docs
- In Practice
	- Often truncate the vector (consider a small number of words with highest weights) for efficiency	
	- Avoid over-fitting
		- need to ensure that the original query terms have sufficiently large weights in feedback
	- Can be used for relevance feedback / pseudo feedback
		- beta (param for centroid of rel docs) should be set to a larger value for relevance feedback



## Feedback in LM: KL divergence
- Generalize query likelihood via KL divergence retrieval model
- Use Query LM: $$ p(w|\theta_Q) $$ instead of c(w,q) in query likelihood
	- Query LM can be estimated in different ways including using feedback
	- If we use $$ p(w|\theta_Q) = \frac{c(w, Q)}{|Q|} $$, then this is identical to query likelihood, thus it's a generalization
- Computing Feedback LM $$ \theta_F $$ using generative mixture model
	- Have a set of feedback docs (clicked by / judged by users)
	- $$ \log{p(F|\theta)} = \sum\sum{c(w,d)\log{[(1-\lambda)p(w|\theta) + \lambda * p(w|C)]}} $$
	- Use Maximum Likelihood: $$ \theta_F = argmax_{\theta}\log{p(F|\theta)} $$
	- lambda small -> more common words 

## Web Search
- Fetch pages via a page's outbound links and then add those to a queue
- Challenges
	1. Scalability
	1. Spam
	1. Dynamics - pages updated very quickly

## Crawler
- Crawling Strategies
	1. Breadth-First
		- balances server load automatically
	1. Parallel crawling
	1. Focused crawling
		- Target a subset of pages
		- Typically given a query
	1. Incremental (repeated) crawling
		- learn from past experience
		- What kind of pages should have a higher priority for recrawling in incremental crawling? 
			- frequently updated / accessed pages
- Crawling Senarios
	1. Initial crawling
		1. Complete crawling
			- e.g. creating a general search engine
		1. Focused
			- e.g. target a certain type of pages
	1. Incremental crawling
		- i.e. incremental updating of the crawl data
		- try to use minimum resource to get the data
- A crawler that only follows hyperlinks cannot identify hidden pages that do not have any incoming links

## Indexer
- Components
	1. GFS
		- Chunk size: 64 MB
	1. MapReduce

### MapReduce
- Framework for parallel programming
- Features
	1. Abstraction
	1. Built-in fault tolerance
	1. Auto load balancing
- Benefits
	1. Minimize effort for creating simple parallel processing tasks
	1. Create index in parallel
- Architecture
	- Input: a number of key value pairs
	- Steps:
		1. Each key value pair will be sent to a map function
		1. Map function will procee the KV pair and generate some other KV pairs with different keys
		1. All outputs from all map functions will be collected and sorted based on keys
		1. The KV pairs with the same keys will be grouped together, so there will be key - value array pairs
		1. Send the pairs to the Reduce function(s). Reduce will then produce a new list of key value pairs
	- Developers only need to specify map function & reduce function

## Retriever
- Challenge of ranking algs for web search
	1. Different info needs
	1. Docs with additional info
	1. Info quality varies
- Anchor text
	- Description of a link that's pointing to another website
	- Can be thought of as a summary
	- Analogy to the title that people put when bookmarking a page
- Authority page
	- Page with no outgoing links and many incoming links
- Hub page
	- Page with no incoming links and many outgoing links

### PageRank
- Main idea: count links to perceive a web page's popularity
	- Assigns weights to the links
	- Smoothing - assign pseudo links to pages that have no ingress links
- Indirect links
	- page being linked by a popular website with many in-links should get more score
- Random jumping (surfing)
	- Idea: at any page, jump to random page with alpha prob, or randomly pick a link to follow with (1 - alpha) prob.
	- Benefits
		1. Prevent zero-outlink nodes to receive all probability
		1. Allow disconnected pages to have non-zero probability
- Normalization does not affect the ranking from PageRank algorithm




# Week 6

## Future of Web Search
- DUS triangle: data, user, service

## Content-Based Filtering
- Recommender is similar to filtering system that makes a binary delivery decision
- 2 ways of deciding whether to recommend (can be combined)
	1. Content-based filtering: item similarity
	1. Collaborative filtering: user similarity
- Components
	- Binary classifier
		- User interest profile
		- Utility function
			- Linear utility = x * #good - y * #bad
				- x high, y low -> aggressive deliver
				- x low, y high -> conservative deliver
	- Feedback
	- Learning
- Challenges
	1. How to make decision
	1. Initialization
	1. Learning
- Reuse retrieval system
	- use retrieval techniques to score docs
	- Use score threshold for filtering decision
	- Challenges
		1. Censored data - judgment only available to delivered docs
		1. Little labeled data - difficult for ML training
		1. Exploration vs Exploitation tradeoff
			- want to explore the space of user interest (e.g. deliver non relevant docs to users) 
			- but too much exploration could lead to exploitation (deviate too much from user’s interest)
- Empirical Utility Optimization
	- Idea
		1. Compute utility for each score threshold
		1. Choose the threshold with max utility
	- Problem: overfitting training sample
		- Solution: Beta-Gamma Threshold Learning
			- Idea: heuristic adjustment (lowering) of utility threshold
			- Set the threshold between optimal and 0 utility
			- $$ \theta = \alpha * \theta_{zero} + (1 - \alpha) * \theta_{optimal} $$
				- $$ \alpha = \beta + (1 - \beta) * e^{-N * \gamma}$$, where $$\beta, \gamma \in [0, 1]$$, and N is the number of training examples. 
				- The larger N is, alpha would be closer to beta, which means the threshold would be closer to optimal theta, i.e. less exploration.
			- Benefits
				1. Address exploration-exploitation tradeoff
				1. Emprically effective


## Collaborative Filtering
- Assumptions
	1. Users with the same interest will have similar preferences
	1. Users with similar preferences probably share the same interest
	1. Sufficiently large number of user preferences are available 
		- Otherwise there would be cold start problem
- Task
	1. Generate a sparse matrix with users as rows and object as columns
	1. Each non-empty entry is a rating of user i on object j
	1. Predict ratings for the empty entries via approximation 
- Idea
	1. Taking average of other users' ratings on the object with weights on different users depending on user similarity
- User similarity measures (will not be biased by the user activity or contents of items)
	1. Pearson correlation coefficient
		- sum over commonly rated items
		- basically measures whether the two users tended to all give higher ratings to similar items or lower ratings to similar items.
	1. Cosine measure
		- treat the rating vectors as vectors in the vector space
		- measure the angle and compute the cosine of the angle of the two vectors
	1. Inverse User Frequency 
		- to emphasize more on similarity on items that are not viewed by many users.
		- similar to IDF


# Week 7

## Text Mining and Analytics Overview
- Text mining almost identical to text analytics
	- Goal of both is to turn text data into high-quality information or actionable knowledge (e.g. help take certain action / make decision)
		1. minimize human effort
		1. helps decision making
	- Mining emphasizes more on the process.
	- Analytics emphasizes more on the result.
- The goal of text mining is also to revert the process of generating text data. 
	- We hope to be able to uncover some aspect of text generation during text mining.
- NLP is the foundation for text mining

## Text Representation
- Text Representation determines what kind of mining algorithms can be applied
- Types
	1. String
	1. Words (array)
		- Word relation analysis; topic analysis; sentiment analysis (opinion mining)
	1. Words + syntactic structure
		- Syntactic graph analysis (structure of a sentence)
	1. Words + syntactic structure + entities & relation
		- Knowledge graph analysis; information network analysis
	1. Words + syntactic structure + entities & relation + logic predicates
		- Integrative analysis of scattered knowledge; logic inference
- Benefits
	1. Improves accuracy of NLP tasks
	1. Useful for many TR and TM application

## Word Association Mining
- A & B have paradigmatic relation if they can be substituted for each other
- A & B have syntagmatic relation if they can be combined with each other
- The two basic and complementary relations can be generalized to describe relations of any items in a language
- Benefits of mining word associations
	1. useful for improving NLP tasks' accuracy
		- related tasks: POS tagging, entity recognition, acronym expansion, grammar learning
	1. useful for text retrieval applications

### Discovering paradigmatic word associations
- Idea: check whether context is similar
- Terms
	- Left context - words occur on the left
		- left1(w): a list of possible words that can occur right before w
	- Right context - words occur on the right 
		- right1(w): a list of possible words that can occur right after w
	- General context - all words in the sentence / words around the word
		- window8(w): all possible words in the window of 8 words around w
- Steps
	1. Let word context be 'Pseudo Document' (imaginary doc)
		- e.g. left1(w), right1(w), window2(w)
	1. Compute similarity between contexts
		- i.e. sim(w1, w2) = sim(left1(w1), left2(w2)) + sim(right1(w1), right1(w2)) + ...
	1. High similarity means 2 words are paradigmatically related
- VSM can model the context for relation discovery
	- Steps
		1. Each word in the vocab is represented as a dimension in VSM
		1. We can represent a 'pseudo doc' (context) of a word (e.g. w1) as a vector
			- i.e. $$ d_1 = (x_1, ..., x_N) $$
			- The value of vector in a dimension $$x_i$$ could depend on the frequencies of the word corresponding to the dimension
		1. Compare 2 vectors to approximate the context similarity
	- 2 pending decisions
		1. What values should be in each vector d1
		1. How to compare the similarity between the 2 vectors
	- Solution: EOWC (Expected Overlap of Words in Context)
		- Idea: 
			- let $$x_i = \frac{c(w_i, d1)}{|d1|} $$ i.e. probability that a randomly picked word from d1 is wi
			- $$ sim(d1, d2) = d1.d2 = x_1y_1 + ... + x_Ny_N $$ i.e. Dot product on the probabilities that the randomly picked words from d1 and d2 are identical
		- Pros
			1. The more overlap the two context documents have, the higher the similarity would be
		- Cons
			1. favors matching one frequent term
			1. treats every word equally
		- Improvements
			1. TR sublinear transformation for computing vector 
				- BM25 is most effective
					- b (0 - 1) controls length normalization 
					- k (> 0) controls the extend of the transformation (upper bound) 
				- to prevent matching one frequent term very well
			1. IDF term weighting for computing similarity
				- i.e. $$ sim = \sum^N_{i=1}IDF(w_i)x_iy_i $$
				- to penalize popular words

# Week 8: Syntagmatic word association mining + Topic mining 

## Entropy - syntagmatic relation discovery
- Idea: check correlated occurrences
	- look for the words that **tend to occur together** with the words
- Word Prediction Formal definition
	- Binary random variable $$ X_w \in [0, 1] $$, X = 1 -> w is present
		- p(x = 1): probability that w is present
		- $$ p(X_w = 1) + p(X_w = 0) = 1 $$
- Entropy
	- Measures randomness of X
	- $$ H(X_w) = \sum_{v \in 0, 1}-p(X_w = v)\log{p(X_w=v)} $$
	- Entropy is between 0 and 1. 
		- 0 entropy -> no randomness i.e. w is either present all the time or not present all the time
		- 1 entropy -> high randomness; whether w is presented is fully random (50% chance)
- High entropy words are harder to predict


## Conditional Entropy
- Intuition: knowing the presence of one word may reduce the randomness of another word's presence
- i.e. $$ p(X_{w1} = 1) \to p(X_{w1} = 1 | X_{w2} = 1) $$
- H(X|Y): entropy of X given we know Y
- Features
	1. $$ H(X) >= H(X|Y) >= 0 $$
- Can be used to capture syntagmatic relation
	- When X is closely related to Y, H(X|Y) is small
	- When X is not related to Y, H(X|Y) is closer to H(X)
- Steps for mining syntagmatic relation
	1. For each word w1
		1. Compute H(w1|w2) for every other word w2
		1. Sort in ascending order
		1. Use threshold to get top-ranked candidates (the smaller the better)
- Problem
	- Cannot compare H(w1|w2) and H(w3|w1) directly (as upper bounds are different). 
		- To compare, we need mutual information. See below.


## Mutual Information (MI)
- Concept: the amount of entropy reduction of X (or Y) due to knowing Y (or X)
- $$ I(X; Y) = H(X) - H(X|Y) = H(Y) - H(Y|X) $$
- Properties
	1. I is non-negative
	1. I is symmetric - I(X;Y) == I(Y:X)
	1. I(X; Y) == 0 iff X and Y are independent
- Can also be used for syntagmatic relation mining
	- syntagmatic related == high MI
- MI measures the divergence of the actual joint distribution from the expected distribution under the independence assumption
	- The larger the divergence is, the higher the MI would be.
	- Can rewrite MI using KL-divergence
- Computation of MI
	- only need to know $$p(X_{W1}=1)$$, $$p(X_{W2}=1)$$, and $$p(X_{W1}=1, X_{W2}=1)$$.
		- Those can be estimated via term frequencies
	- Smoothing: add small constants to accommodate 0 counts


## Topic Mining
- Topic: main idea discussed in text data
- Application examples
	1. What are current research topics?
	1. What are Twitter users talking about today?
- Tasks
	1. discover k topics
	1. figure out which docs cover which topics
- Formal Definition of Topic Mining
	- Input
		- A collection of N text docs C = {d1, ..., dN}
		- Number of topics k
	- Output
		- k topics $$ \theta_i ... \theta_k $$
		- Coverage of topics in each doc di $$ {\pi_{i1}, ... \pi_{ik}} $$
		- $$\pi_{ij}$$ = prob. of di covering topic $$ \theta_j $$
- Question: how to define topic $$ \theta $$?


## Term as Topic
- Let $$ \theta $$ be an individual term
- Mining steps
	1. Get candidate terms from C
	1. Pass terms to a scoring function
	1. Pick k terms as topics with the highest scores but try to minimize redundancy
- Computing topic coverage
	- $$ \pi_{ij} $$ = count of term j in doc i / count of all topics in doc i
- Problems
	1. Cannot represent complicated topics
	1. Hard to capture variations of vocabulary (e.g. related words)
	1. Word sense ambiguity (same word can have different meanings)


## Probabilistic Topic Models
- Improved idea: topic = **word distribution**
- Fixing the problems above via
	1. using multiple words
	1. adding weights on words (probabilities)
	1. splitting an ambiguous word (the same word will have different probs in different distributions)
- Probabilistic Topic Mining
	- Input
		- A collection of N text docs C = {d1, ..., dN}
		- Vocabulary set V = {w1, ... wM}
		- Number of topics k (can be a hypothesis)
	- Output
		- k topics, each a word distribution $$ \theta_i ... \theta_k $$
		- Coverage of topics in each doc di $$ {\pi_{i1}, ... \pi_{ik}} $$
		- $$\pi_{ij}$$ = prob. of di covering topic $$ \theta_j $$
- To compute the result, we can use Generative Model
	- Idea:
		1. design a model to represent how our data is generated $$ P(Data | Model, \Lambda) $$
			- The parameters will be denoted as captialized lambda $$\Lambda$$
			- $$\Lambda = (\{\theta_1 ... \theta_k\}, \{\pi_{11}, ... \pi_{1k}\}, ... \{\pi_{N1}, ... \pi_{NK}\}) $$
		1. We can use ML estimate to fit the new generative model to our data
			- $$ \Lambda^* = argmax_{\Lambda}p(Data | Model, \Lambda) $$
				- i.e. find the parameter set that gives our data set the highest probability 
		1. The recovered parameter values from the best generative model would then become the output of the topic mining alg. i.e. the topic
- Problem: how to get rid of the common words?
	- Solution: see mixture model in week 9 

## ML estimate
- One way to estimate the parameters from generative model
	- choose the parameters where the data likelihood is maximized
- $$ \hat{\theta} = argmax_{\theta}p(X | \theta) $$ (check the last section for example)

## Bayesian estimation 
- Another approach to estimate the parameters from generative model besides ML estimate
- Problem of ML estimate
	1. does not work on a small set of data (may lead to overfitting)
	1. unseen words would have 0 probability
- Baysian uses the data and the prior knowledge ($$ p(\theta) $$) about the data
- Bayes Rule 
	- $$ p(X|Y) = \frac{p(Y|X)p(X)}{p(Y)} $$
- Estimation (based on bayes rule)
	- $$ \hat{\theta} = argmax_{\theta}P(\theta|X) = argmax_{\theta}P(X|\theta)P(\theta) $$
		- AKA MAP (Maximum a Posteriori) estimate - maximize the posterior estimate (instead of only $$P(X|\theta)$$)
- Benefits
	1. Allows for inferring any 'derived value' from $$\theta$$


# Week 9

## Mixture of Unigram LMs
- Probability of generating w from a mixture generative model: $$ p(w)=p(\theta_d)p(w|\theta_d) + p(\theta_B)p(w| \theta_B) $$


## Mixture Model estimation
- Can be used to factor out common words
- 'Collaboration' and 'Competition'
	- Assume d = w1w2, $$ p(\theta_d) = p(\theta_B) = 0.5 $$ i.e. 50% chance to choose between background LM and doc LM to generate the doc
	- $$ p(d|\Lambda) = 1/4 * (p(w_1|\theta_d) + p(w_1| \theta_B)) * (p(w_2|\theta_d) + p(w_2| \theta_B)) $$
	- Constraints:
		- $$ p(w_1|\theta_d) + p(w_2| \theta_B) = p(w_1| \theta_d) + p(w_2| \theta_d) = 1 $$
	- Note: if x + y = constant, then xy is maximized when x = y
	- Thus to maximize the product,
		- $$ p(w_1|\theta_d) + p(w_1| \theta_B) $$ should equal to $$ p(w_2|\theta_d) + p(w_2| \theta_B) $$
		- i.e. $$ p(w_1|\theta_d) = p(w_2| \theta_B) $$, and $$ p(w_1| \theta_B) = p(w_2|\theta_d) $$
- Behaviors
	1. Every component model attempts to assign high probabilities to highly frequent words in the data (to “collaboratively maximize likelihood”)
		- Different component models tend to “bet” high probabilities on different words
		- If $$ p(w1|\theta_B)> p(w2|\theta_B) $$, then $$ p(w1|\theta_d) < p(w2|\theta_d) $$
	1. High frequency words get higher $$ p(w|\theta_d)$$
	1. The probability of choosing each component “regulates” the collaboration/competition between the component models
		- If one distribution has larger probability to be chosen, then high probabilities in that distribution would be increased


## Expectation-Maximization Alg
- Intuition task
	- Computing the estimate of word distribution (i.e. $$p(w|\theta_d)$$, word probabilities within the model) in $$ \theta_d $$ given all other parameters, including
		1. Mixture model with a background distribution, and $$ p(\theta_B) = 0.5 $$
		1. Assume we know which words are from which distribution
	- Solution: count all words that are from the distribution d and calculate probabilities based on counts and normalize
- In practice the 2nd assumption above is not known, but we can infer which words are from which distribution
	- We have some prior to help infer
		- Prior means knowledge possessed before observing a word
		- i.e. probability of choosing which distribution to use i.e. $$ P(\theta_d) = 0.5 $$
	- Then by Bayes rule, we combine prior with likelihood i.e. compare $$ p(\theta_d)p(w|\theta_d) $$ and $$ p(\theta_B)p(w| \theta_B) $$ to see if w is more likely to be generated from d or from B.
- Alg steps
	1. Assign hidden variables z to each occurrences of words indicating whether the word is from background model B or d
		- z = 0 -> w is generated from d  topic
		- z = 1 -> w is from background model
	1. Initialize $$ p(w|\theta_d) $$ with random values
	1. Iteratively improve the estimates of hidden parameters z using 
		1. E-step: to augment the data with additional information
			- Predicts values of hidden variables via normalized probability via Bayes rule based on the current values of $$ p(w|\theta_d) $$ i.e. predict the value of z (whether the word is from B or d)
		1. M-step: Revise the values of $$ p(w | \theta_d) $$ based on the predicted z values from E-step via ML estimator
			- Maximizes the joint likelihood given the predicted values of unseen data
	1. Stop when likelihood does not change
- EM alg converges to a local maximum


## PLSA - Probabilistic Latent Semantic Analysis
- Useful for mining multiple topics
	- It's a mxture model with k unigram LMs (k topics)
- Use cases
	1. Clustering of terms & docs
	1. Associate topics with different contexts
- Input: C (collection of docs), k (num of topics), V (vocab set)
- Output: k topics, proportion of each topic in each doc
- Generating text with multiple topics
	- To generate a document, a distribution of topic weights (multinomial distribution) is assumed, which is considered part of the model.
	- To generate a word, a topic is drawn from the document's topic weight distribution, and a word is drawn according to the topic's word distribution.
	- $$ p(\theta_B) = \lambda_B $$ i.e. the probability of choosing background model to generate w
	- Probability of observing w in d is the sum of probabilities over all distributions
		- $$ p_d(w) = \lambda_Bp(w|\theta_B) + (1 - \lambda_B)\sum^k_{j=1}\pi_{d,j}p(w|\theta_j) $$
			- this will be used in E-step
		- The coverage $$ \pi $$ and word distribution $$ p(w|\theta_j) $$ need to be estimated
- Can use EM to compute the ML estimates for the above unknown parameters

### EM Computation
- E-step (augment)
	- $$ Z_{d,w} \in {B, 1, 2, ... k} $$ denoting which distribution is the word w in doc d generated from 
	- $$ p(z_{d,w} = j) = \frac{\pi_{d,j}p(w|\theta_j)}{\sum^k_{j=1}\pi_{d,j}p(w|\theta_j)} $$
		- Use Bayes rule and apply the formula from the last section
- M-step
	- Revise the $$ p(w|\theta) $$ values based on updated $$ Z_{d, w} $$  


### PLSA with Prior knowledge
- Can incorporate prior knowledge as priors
- Can use the MAP estimate
	- $$ \Lambda^* = argmax_{\Lambda}p(\Lambda)p(Data, \Lambda) $$

### Best practices
- large collection of documents to train PLSA:
	- Train PLSA on a small subset collection of documents and use the model to initialize, and for other documents randomly initialize the documents' topic weights

## LDA - Latent Dirichlet Allocation
- PLSA Issues
	1. Not a generative model
	1. Too many parameters; complex model
		- Many local maxima -> overfitting
- LDA
	- Generative model for PLSA via a Dirichlet prior
	- Bayesian version of PLSA


# Week 10: Text Clustering & Text Categorization

## Text clustering
- Task: group similar objects together 
- Users need to specify the perspective ('bias' / criteria) for assessing similarity
- Why clustering?
	1. Useful for text mining and exploratory text analysis
	1. Get a sense about overall content
	1. Link text objects
- Use Cases
	1. Define concept, theme, topic
	1. Clustering websites
	1. Hierarchy generation
- Model-based clustering
	- difficult to substitue a different similarity measure

## Similarity-based clustering
- Objectives:
	1. Maximize intra-group similarity i.e. objects within the group should be similar
	1. Minimize inter-group similarity i.e. objects in different groups should not be similar  
- Strategies
	1. Progressively construct a hierarchy of clusters
	1. Start with an initial tentative clustering and iteratively improve it
		- e.g. k-means

### HAC (Hierarchical Agglomerative Clustering)
- Steps
	1. Gradually group similar objects together in a bottom-up fashion to form a hierarchy
	1. Stop when some stopping criterion is met
- Group similarity measures
	1. Single-link algorithm: s(g1,g2)= similarity of the closest pair
		- optimistic; loose cluster; sensitive to outliers
	1. Complete-link algorithm: s(g1,g2)= similarity of the farthest pair
		- check the worst case; tight cluster; sensitive to outliers
	1. Average-link algorithm: s(g1,g2)= average of similarity of all pairs

### k-means
- Idea
	- Represent each text object as a term vector and assume a similarity function defined on two objects
	- Similar to EM alg.
- Steps
	1. Start with k randomly selected vectors and assume they are the centroids of k clusters
	1. repeat until converge:
		1. Assign every vector to a cluster whose centroid is the closest to the vector
		1. Re-compute the centroid for each cluster based on the newly assigned vectors in the cluster
- Converges to local minimum

### Clustering Evaluation
1. Direct
	- Assess 'correctness'
1. Indirect
	- Compare the performance of the clustering system and the baseline in terms of any performance measure for the application



## Text categorization
- Task: classify a text object into 1 or more of predefined categories after training the model with a labeled training set.
- Categories
	1. “Internal” categories that characterize a text object
	1. “External” categories that characterize an entity associated with the text object
		- e.g. author, time
- Use cases
	1. enrich text representation
		- Text can now be represented in multiple levels (keywords + categories)
		- facilitate aggregation of text content
	1. infer properties of entities associated with text

### Methods
1. Manual
	- Idea: Determine the category based on rules that are carefully designed to reflect the domain knowledge about the categorization problem
	- Works well when categories are easily distinguished and very well defined
	- Problems
		1. Labor intensive
		1. Cannot handle uncertainty in rules; not robust
1. 'Automatic'
	- Input:
		1. Annotated data sets with category labels
		1. A set of features that can potentially provide a “clue”
	- Processer:
		- Use machine learning to learn “soft rules” for categorization
			- Figure out which features are most useful
	- Output:
		- The classifer to predict category
	- Generative vs Discriminative
		- Generative classifiers learn what the data “looks” like in each category
			- compute p(Y|X) based on p(X|Y) and p(Y) by using Bayes Rule
			- e.g. Naive Bayes
		- Discriminative classifiers (learn what features separate categories)
			- attempt to model p(Y|X) directly
			- e.g. Logistic Regression, SVM, kNN

### Naïve Bayes
- Idea:
	- $$ p(\theta | d) = \frac{p(\theta) * p(d | \theta)}{p(d)} = \frac{p(\theta) * p(d | \theta)}{\sum^k_{j=1}p(d|theta_j)p(\theta_j)} $$
- Steps
	1. Given a list of documents with categories labeled as training data
	1. Approximate $$p(\theta)$$ and $$p(w|\theta)$$ from the training data
		- $$ p(\theta) = $$ num of training docs belong to category i / num of all training docs
		- $$ p(w|\theta) = $$ word count of w in the training docs belong to category i / global word count w in all training docs
	1. Classifier: $$ category(d) = argmax_i(p(\theta_i|d)) = argmax_i(\log{p(\theta_i)} + \sum_{w \ in V}c(w,d)\log{p(w|\theta_i)}) $$
- Smoothing
	- Why needed?
		1. Address data sparseness
		1. Incorporate prior
		1. Achieve discriminative weighting (i.e., IDF weighting)

# Week 11

## Discriminative Classifer

### Logistic Regression
- Input
	1. Predictors X with M features
- Goal: Modeling p(Y|X) directly 
	- Process: estimate parameters $$\beta$$ with ML estimate
- Output: Binary classifier outputing Y, which indicates if category of a d is $$theta_1$$ or $$theta_2$$
- can be regarded as a generalization of a Naïve Bayes

### KNN
- Idea: 
	1. Find k examples in the training set that are most similar to the text object to be classified
	1. Assign the category that is most common in these neighbor text objects
- Need a similarity function

## Text Categorization Evaluation
- Idea: comapre the system categorization decisions with the human-made categorization decisions
- human answer: +/-; system answer: y/n
	- TP: true positive - y(+)
	- FP: false pos - y(-)
	- FN: false neg - n(+)
	- TN: true neg 
- Classification Accuracy 
	- num of correct decisions / num of decisions made
	- cons
		1. some decisions have more weights than others
		1. not effective with imbalanced test set
- Per doc evaluation
	- Precision: TP / (TP + FP)
		- When system says 'yes', how many are correct?
	- Recall: TP / (TP + FN)
		- How many categories out of total categories of the doc are captured?
- Per category evaluation
	- Precision: TP / (TP + FP)
		- When system says 'yes', how many are correct?
	- Recall: TP / (TP + FN)
		- How many docs out of all docs within the category are captured?
- F-measure
	- $$ F_1 = \frac{2PR}{P+R} $$
- Micro-averaging
	- pooling all decisions
- Macro-averaging
	- average over precision / recall / f
- Can aggregate over the categories


## Opinion Mining and Sentiment Analysis
- Opinion: a subjective statement describing what a person believes or thinks about something
- Opinion representations
	1. holder
	1. target
	1. content
	1. context
	1. sentiment
		- What does the opinion tell us about the opinion holder’s feeling
- Why mine opinion?
	1. decision support
	1. understand people e.g. preferences
	1. 'voluntary survey' - gather information
- Task
	- Input: text data
	- Output: A set of opinion representations -> opinion sentiment

## Sentiment Classification
- Input: opinionated text object
- Output: A sentiment tag/label
	- Polarity analysis
	- Emotion analysis
- Any text categorization method can be used
- Additional features needed for sentiment tagging
	1. Character n-grams
		- Unigrams are often very effective, but not for sentiment analysis
	1. Word n-grams
		- unigrams
			- check one word at a time
		- bigrams
			- check 2 consecutive words at a time
		- trigrams
	1. POS (part of speech) tag n-grams
	1. Word classes
	1. Frequent patterns in text
	1. Parse tree-based
	1. Pattern discover alg.

## Feature construction
- Domain knowledge, error analysis, and machine learning are all useful for text categorization


# Week 12: Text-based prediction

## Joint mining of text and non-text data
- Non-text data provides context for text mining
- Text data help interpret patterns discovered from non-text data

## Contextual Text Mining
- Why?
	- Text often has rich context information
		1. direct context
		1. indrect context
	- Context can be used to
		1. Partition text data for comparative analysis
		1. Provide meaning to the discovered topics


## CPLSA (Contextual Probabilistic Latent Semantic Analysis)
- Extension of PLSA
- Idea
	- Explicitly add interesting context variables into a generative model
		- Context influences both coverage and content variation of topics
	- Model the conditional likelihood of text given context i.e. p(data | context variables)
- Use cases
	1. analyzing the impact of an event
		- compare the text content before an event and the content after the event
	1. Discovering temporal trends of topics in text
	1. Revealing how the coverage of topics in different locations evolves over time
		- e.g. the trending topics in different countries in Twitter with the location information provided


## Network Supervised Topic Modeling
- The context of a text article can form a network
	- text data can be associated with nodes (NetPLSA), edges, paths, subnets, etc.
- Benefits of joint analysis of text and network context
	1. Network imposes constraints on topics in text
	1. Text helps characterize the content associated with each subnetwork
- Idea
	- Context network imposes constraints on topics & model parameters
	- The text at two adjacent nodes of the network tends to cover similar topics
	- Topic distributions are smoothed over adjacent nodes
- Use cases
- NetPLSA 
	- leverages the power of both the text and the network structure to mine topics




## Casual Topic Mining
- Mine the "cause" (strong correlated event) of an event 
- Input
	1. Text (text stream)
	1. Time series
- Iterative Causal Topic Modeling with Time Series Feedback
	- Idea: do an iterative adjustment of topic, discovered by topic models using time series to induce a product.
- Use cases
	1. discovering topics whose coverage in Twitter has strong correlations with airline prices
- Measuring casuality
	- Granger Causality Test is often useful
