# Course Overview

## Objectives
Upon completing the course, you will be able to:
1. Make more effective visualizations for data.
1. Be able to choose the most appropriate visualization method for a given type of data and task.
1. Understand how fundamental principles of design and human cognition inform effective visualizations.
1. Utilize the popular scientific visualization application ParaView.
1. Work with Python and PyVista to visualize scientific data.
1. Understand current research directions and open problems in scientific visualization.


# What's Scientific Visualization

## Disambiguate 
- Data visualization: the techniques used to communicate data or information by encoding it as visual objects
- Information visualization: 
	- The idea of displaying information that doesn't have some inherent spatial meaning to it.
	- A process of transforming data and information that are not inherently spatial into a visual form
	- The layout wouldn't be natural (e.g. would not have order in time / space)
- Scientific visualization
	- Using visualization techniques to convey information about data that has some natural spatial component to it
	- Typically focuses on spatial data generated by scientific processes
	- Info. Vis. vs Sci. Vis.
		- More similar than different
		- Usually scientific visualization means that you're looking at data from a physical or a metric space
		- Usually information visualization means that you're looking at more abstract data with no natural location.
- Visual analytics
	- The field that's concerned with developing tools that a human analyst would use, that combines visual elements along with more advanced data mining elements to allow you to analyze data and come up with conclusions. 
	- science of analytical reasoning faciliated by interactive visual interfaces

## Topics
1. Terrain visualization
	- It is a fine-grained detailed visualization of some sort of surface
1. Volume visualization (spatial e.g. core of an object)
1. Isosurface
	- e.g. MRI data
1. Flow visualization
1. Tensor field visualization
	- Going towards all directions
	- e.g. Diffusion tensor MRI
1. Bioinformatics visualization

## Why visualization
1. Help other people carry out tasks more effectively
	- What tasks?
		- Reveal patterns and generate hypotheses
		- Explore data and make decisions / answer questions
1. Communicate ideas and persuade / inspire

### When to avoid visualization
- Visualization can be misleading  
- Shouldn't use visualization when it's not needed (i.e. better done without visualization / self-explanatory)
	- Can do does not mean need to




# Representing Colors

## Digital Images
- Raster: grid of addressable image elements called pixels
- Pixel: the smallest addressable element in that raster
	- i.e. can specify one color on a pixel

## Display techs
1. LCD: Liquid Crystal Display 
	- A transmissive technology that uses light-emitting diodes (LED) as a backlight or edge light. That light gets filtered through a substrate to produce the colors that you see
	- Multiple layers. Starting from backlight and then filtered to RGB
	- Benefits
		1. Cheaper
		1. More energy efficient
1. OLED: Organic light emitting diode
	- Different type of diode
 	- Don't need LED backlight, the diode is emissive
 		- use mix of blue / orange-yellow OLED emitters
		- Benefits
			1. Deep black levels
				- In LCD, the bright light in one pixel may spill over to other pixels, impacting surrounding area's light
			1. Excellent viewing angles
			1. Fast refresh
			1. Can potentially be manufactored on flexible substrates 
				- Can generate curved displays

## RGB Color Space
- Any set of wavelengths can serve as primaries e.g. RGB
	- A 3 wavelength color space cannot produce all the colors a person can see
- Each pixel color is specified by a tuple of 3 numbers (of the primaries)
- Color channel: each of the R, G, B values

### Alpha Channel
- RGB + Alpha
- Alpha means opacity
	- 1.0 means opaque
	- 0.0 means transparent
- Useful for simulating semi-transparent surfaces and compositing
	- e.g. MRI images


## Intensity
- the value of a channel. Typically between [0.0 - 1.0] - 0.0 means no light, and 1.0 means full intensity
- Display intensity - represented by 8-bit
	- Each channel is an 8-bit unsigned int
	- Expressions
		1. Decimal: 0 - 255
		1. Hex: 0x00, 0xFF
	- Can convert to 0 - 1 scale by dividing 255
- People can percive far more luminance values than 255.
- HDR (high-dynamic range) displays may use different representations to expand the range of luminance values
	- e.g. HDR TV uses 10 bits or 12 bits per color channel
- In code, we should keep color values as at least 32-bit floats to
	1. Reduce impact of quantization on the colors
		- **Quantization**: when we have a large set of values and we're mapping them into a smaller set of values. 
			- e.g. From continuous to discrete
			- Could happen during displaying images / storing images as files
	1. Allow users to query for the original numerical value to prevent quantization / display differences

## File formats
- PNG (Portable Network Graphics) formats
- Structure
	1. Header
	1. Chunks
		- Each chunk data
			1. Length
			1. Chunk type (color type)
				1. grayscale
				1. truecolor
				1. Indexed
				1. Grayscale and alpha
				1. Truecolor and alpha
			1. Data
			1. CRC code

## HSV (Hue, saturation and value) Color Space
- an alternative color space; equivalent to RGB in the colors that it can represent
- easier to pick colors - UI/UX is a color wheel (used in drawing tools)
- Components
	1. Hue [0 - 360]: angle about color wheel
		- e.g. 0 == red, 60 == yellow, 120 == green, 240 == blue
	1. Saturation [0 - 1]: distance from gray
		- distance from the center of wheel
	1. Value [0 - 1]: distance from black
		- 0 == black, 1 == bright
		- changes the whole wheel

### RGB to HSV conversion 
```
Let maxRGB = max(R,G,B)
Let minRGB = min(R,G,B)
S = (maxRGB - minRGB) / maxRGB
V = maxRGB
H depends on which color is maxRGB
```



# Perceptually-defined color spaces

## Perceptual Color Space
- Color is a perceptual phenomenon
	- instead of a physical property
- Light
	- Photon energy is propotional to frequency
	- Frequency is inversely propotional to 
	- Intensity is related to the number of photons received 
- Eye Cone cells
	- 5 million cells
	- 150 hues; can separate 7 million shades of color
	- Types
		1. L cone: sensitivity on yellow or red
			- 63% 
		1. M cone: sensitivity on green
			- 31%
		1. S cone: sensitivity on blue
			- 6%
- Eye Rod
	- measures intensity; perception of brightness
	- 80 million 
	- sensitive, shut down in daylight
- $$ V(\lambda) $$ represents luminosity
- Metamor
	- Same perceived color from different distributions

## Color Space
- A way to specify color
- A color corresponds to some stimulation of L, M, S cones
- To create a color space, given there's no wavelength stimulates only 1 cone, we need a mixture of wavelengths
	- Steps
		1. Choose 3 discrete wavelengths as primaries
		1. Mixing 3 wavelengths at different intensities

## CIE RGB Color Space
- Primaries
	1. blue
	1. green
	1. red
- Based on color matching experiment. Contains negative values.

### CIE XYZ Color Space
- Uses linear transformation to remove negative values.

### CIE xyY Color Space
- Use normalized chromaticity values on top of XYZ color space.
- Conversion
	- $$ x = \frac{X}{X+Y+Z} $$
	- $$ y = \frac{Y}{X+Y+Z} $$
	- $$ Y = Y $$


### CIE xy Chormaticity Diagram
- Depicts all colours visible to an average human
- Horsehoe shape
- Boundary is the spectral locus, which consists of all colors associated with a single wavelength (monospectral light)
- You are seeing the chormaticity diagram shown on a computer screen, which means it was generated using the sRGB color space. Since the sRGB color space cannot display all possible colors in the xyY color space, that means some of the colors shown in the image are incorrectly represented.

## Gamma Correction
- RGB gamma correction: $$ f(c) = c^{\frac{1}{\gamma}}$$
- sRGB gamma correction: $$ f(c) = 1.055c^{\frac{1}{\gamma}} - 0.055 $$




# 3D CG

## Shading
- **Shading** the process of determining what colors we're going to use on the pixels when we render them

### Simple Light Source Models
1. Point source
	- e.g. lamp
1. Directional source
	- e.g. sunlight to earth
1. Ambient light

### Phong Reflection Model
- A simple model that can be computed rapidly
- Components
	1. Diffuse - rough surface
	1. Specular - smooth surface
	1. Ambient
- Vectors from a point on surface
	1. L - To light
	1. V - To viewer
	1. N - Normal vector
	1. R - To perfect reflector
- Summing up 3 components would give the colours to represent the light reflected in RGB values
	- $$ I = k_ai_a + \sum_{m \in lights} (k_d(L_m * N)i_{m,d} + k_s(V \dot R_m)^ai_{m,s}) $$
- An attenuation factor can be applied to the diffuse and specular terms

### Blinn-Phong Reflectance Model
- Utilized a normalized halfway vector between viewer and light
	- $$ H = \frac{L + V}{|L + V|}$$
- Replaces $$(V \dot R)^a$$ with $$(N \dot H)^b$$
	- Exponent b ccan be picked to match what users want
	- High exponent b would let Blinn-Phong look the same as Phong
- Advantages
	1. More efficient operations
	1. Closer to physical lighting


## Rendering and Visualization
- Projection
- Shading

### Hidden surface removal
- HSR determines what objects are in front of other objects in the scene and occlude those objects.
- How it's been determined?
	1. **Z-buffer** a 2D array of Z values on each pixels being displayed
	1. Fragments will be iterated to record Z values on the 2D array if the Z values are smaller than the ones currently in the array entries
		1. i.e. the part of the fragment with the least Z-values is retained 
- Z-fighting
	- The engine fails to determine which surface is closest 
	- occurs when surfaces are close to co-planer (i.e. almost on the same plane) 
	- $$ \delta z = \frac{f - n}{B} $$
	- Ways to fix? 
		1. Separate the surfaces apart
		1. Move the near and far planes closer s.t. delta Z (bucket depth) would be smaller


## Scalar Field
- A scalar is a single quantity (a number)
- A **scalar field** assigns a scalar to every point in a given space
- Pseudo-coloring
- Choropleth
- **Contour Lines** are lines in which the function has a constant value and that banding then allows people to better localize the values that they're looking 

### Designing a Color Map Options
1. Color Table
	- like a hashtable where key is number and value is color
	- Suitable for small set of keys
1. Transfer Function
	- More common approach; breaks up colors into RGB scalars
	- A **Transfer Function** defines colours at certain scalar values
		- The points are called knots
		- Then interpolation is performed to decide and assign colors to the knots

### Colormap Design Advice
1. Design for accessibility - consider colorblind
1. Decide whether to use diverging colormap based on the data set
1. See if audience has a standard e.g. geography
1. Perceptually uniform colormap typically is a good choice




# Data Taxnomy

## Data types (structural or math interpretation of data)
1. Items
1. Attributes
	- Properties of items e.g. measures, observations
1. Links
	- Express relationships
1. Positions
1. Grids

## Dataset types
- Some ways of organizing the data

### Examples
1. Tables (tabular data)
	- Components:
		1. Items
		1. Attribute
	- Flat table (e.g. SQL table)
		- One item per row
		- Each column is an attribute
	- Multidimensional table
		- Indexing based on multiple keys
1. Networks and Graphs
	1. Trees
	1. Directed / undirected graphs
	1. Network
		- graph-based but has a lot of additional information
1. Fields (spatial)
	- underlying continuous function that we typically are going to be discreetly sampling using a structure like a grid
	- Each cell in the grid contains attributes
	- Spatial fields
		1. scalar
		1. vector
		1. tensor
1. Geometry (spatial)
1. Cluster, set, lists

## Attribute 

### Attribute types
1. Categorical 
1. Ordered
	1. Ordinal
	1. QUantitative

### Ordering direction
1. Sequential
1. Diverging
1. Cyclic



# Interpolation
- Interpolation: a data fitting technique that matches the observed data exactly
- Data fitting
	- Why data fitting? To fill in unknown values in the domain based on existing values
- Aliasing
	- distortion or artifact that results when a signal reconstructed from samples is different from the original continuous signal.
- Extrapolation
	- Inferring unknown values beyond the range of known values
	- Interpolation methods typically work poorly on extrapolation

## Linear Interpolation
- Why linear interpolation?
	- Simple conceptually and computationally
- Don't use it if you know the underlying function is non-linear
- Given v0 (f(p0)) and v1 (f(p1)), $$f(t) = (1-t)v0 + tv1 $$ 
- Given p0, pi, p1, $$t = \frac{pi - p0}{p1 - p0} $$

### Bilinear Interpolation
- Use case: 2-dimensional domain unknown values
- Do linear interpolation 2 times
	1. fix y's and interpolate over x values
	1. fix x and interpolate over y
- Can interpolate first from either axis

### Trilinear Interpolation
- Do linear interpolation 3 times
	- Interploate over x, y, z axis


## Barycentric Interpolation
- Use cases
	1. Scattered data
		- Triangulate first and then interpolate
	1. Triangulated domain
- Can interpolate over not only triangles, but any simplex
	- A simplex is a convex hull of k plus 1 points in a k-dimensional space. 
	- e.g. 3-simplex is a triangle, 4-simplex is a tetrahedron
- Given 3 vertices a, b, c from a triangle and a point p inside the triangle
- Need to calculate the lambda values, the relative coordinates respect to the vertices of triangle 
	- Via solving the following equations:
		1. $$ p = \lambda_1a + \lambda_1b + \lambda_1c $$, where p, a, b, c are cartesian coordinates
- Interpolation: $$ f(p) = \lambda_1f(a) + \lambda_1f(b) + \lambda_1f(c) $$


## Scattered data interpolation
- Scattered data is irregularly sampled

### Shepard's Method

### Radial Basis Functions
- Radial function: any function that is dependent on distance from some center. Denoted as $$ \phi $$
- Interpolation function
	- $$ f(x) = \sum_{i = 1}^NW_i\phi(x, p_i) $$
- Weights can be computed by solving the system of linear equations
	- A = $$ \begin{bmatrix}
		\phi(p1, p1) & ... & \phi(p1, pN)  \\ 
		\phi(p2, p1) & ... & \phi(p2, pN) \\
		... & ... & ... \\
		\phi(pN, p1) & ... & \phi(pN, pN)
	\end{bmatrix} $$
	- Aw = f, where w are weights and f are sample function values


# Contour

## Match Square



# Domain Modeling

## Domain Discretization
- Partition the domain into cells
- There's often a difference between the mesh that contains the scientific data and a mesh that you'll use to render.

## Terminology
- Data can be separated into meshes (grids)
	- Mesh example: triangle, tetrahedral
- Cell
	- Components
		1. Vertices
		1. Edges
		1. Faces

## Mesh / Grid
- Each meshes will be composed on a set cells s.t. no 2 cells overlap, and the cells cover all domain
- A mesh is not a raster. Instead, it is a breakdown of a surface (any surface) into a set of smaller cells that can be operated on more easily (for example, to project down to 2d). Your mesh is also known as a grid; it partitions our complex data domain into cells which can provide interpolation over a small, simple-shaped spatial region e.g. triangles in a 2d space.
	- Raster is the pixelated shapes generated from rasterization
- The data for a mesh can be divided into 2 components
	1. Geometry: the positional data about the vertices i.e. coordinates
		- Can be structured or unstructured
			- Structured: uniform
			- Unstructured: rectilinear
	1. Topology: specifies how those vertices are connected together. i.e. connectivity
		- Can be structured or unstructured
			- Structured: uniform, rectilinear
			- Unstructured: Unstructured
- A triangulated irregular network can be classified as
	1. Polygonal mesh
	1. Surface mesh
	1. Unstructured mesh

### Grid types
1. Linear
1. Rectilinear
1. Curvilinear
1. Unstructured

## Surface Mesh 

### Properties
1. Manifold
	- Every edge connects exactly 2 faces
1. Orientable
	- the vertices for a triangle in a specific order
1. Watertight
	- orientable and manifold
1. Boundary
	- Some edges bound only one face
1. Ordered
	- Vertices in counter clock wise order when viewed from normal
1. Genus
	- Number of handles the surface has

### Euler Characteristic
- $$ V - E + F = 2 * (1 - G) $$


## Data Structure for meshes

### Halfedge data structure
- Maintaining 3 tables - vertex, face, halfedge
- Useful for 
	1. Gathering vertices
	1. One-ring traversal
		- One-ring traversal is when we gather up all of the neighboring vertices for a given vertex.
- Advantages
	1. Simple, efficient traverals of vertex neighborhoods
	1. Can be applied to any polygonal mesh


## Terrain Visualization
- A DEM is a digital elevation model. It is an image-based format encoding elevation data about a chunk of terrain.
	- Raster data set
	- A surface mesh
	- A uniform mesh
- A TIN is a triangulated irregular network. It is a triangle surface mesh that encodes information about a terrain.
	- Triangulated surface mesh in unstructured grids
- LiDAR: method for measuring distance by shooting laser light and detecting the reflection time

### TIN Streaming to generate DEM
- Generate DEM from LiDAR via TIN streaming and processing on the fly
- Steps
	1. Don't need to sort the data; instead read data from disk directly without ordering
	1. Finalizer (data pre-processing)
		- Purpose
			1. Compute bounding box
			1. Create finalization grid
				- Count number of points per cell
			1. output finalized points with tags
	1. Compute TIN triangles in places where all points have already arrived
		- If not all surrounding data is arrived, then we keep those in memory 
		- Approach for generating TIN triangles: Delaunay Triangulation
			- Triangulation in which every triangle has an empty circumscribing circle
				- Circumscribing circle - a circle passing through all vertices of the triangle
				- Empty circumscribing circle means no other circumscribing circle is within the circle
			- This can help avoid trigangles with small angles
				- Avoid computation error during interpolation
			- Alg: Incremental Point Insertion
		- After triangulation, output and deallocate
	1. Raster the chunk and output
		- Rasterization
		- Resampling
			- Want to move from having irregularly spaced samples in the TIN to having a uniform grid of samples that form the DEM
			- Via interpolating in uniform grid
			- Potential interpolation methods
				1. Shepard / RBF via radial sampling
				1. Kriging
				1. Using triangle interpolation of TIN triangles
				1. Higher order interpolation



# Marching Cubes
- To achieve contouring in 3D
	- Generates iso-surface instead of iso-line in 2D
- Most famous alg. for generating iso-surface
- Idea
	- Performing matching squares in 3D
	- Use isovalues to determine if there's an iso-surface cutting the cube cell by comparing with all 8 vertices' sample values 
- Bipolar Edge: one vertex's sample value is greater than the isovalue while the other is less than the isovalue
- Since there are 8 vertices per cube
	- 256 combinations of configurations
	- Can be categorized into 15 cases via
		1. Complementarity
		1. Rotation
- Disadvantages
	1. Too many triangles generated
	1. Ringing artifact - generate rings
	1. Ambiguity
		- Largest downside. Similar to matching squares

## Consistency

## Correctness




# Dual Methods
- Another class of contouring algorithms

## Dual Matching Squares
- Finding the midpoint between the 2 vertices at the intersection between isoline and the boundary (edge)
- 14 cases can be put into 2 categories
	1. case 5 and case 10
	1. remaining
- Advantages
	1. Better at reproducing sharp features inside the cell
	1. Simpler to implement




# Volume Rendering
- Definition: Any rendering process which maps from volume data to an image without introducing binary distinctions / intermediate geometry


## Ray Casting

## Volume Shading
- Use Blinn-Phong Reflection Model
	- Ambient term is not optional for volume rendering
- We will use the gradient vector as the normal vector for volumetric shading
- Normal vector
	- Can use gradient vector
	- Can compute or approximate gradient vector via central difference formula

## Compositing
- Definition: the action of accumulating color and opacity along the ray on which we're integrating
- Compositing allows us to accumulate that color and opacity and generate an image using volume rendering techniques.
- It is one way for us to approximate the volume rendering integral
	- Why approximating?
		1. Lack of computational resources for visualization
		1. Many sources of error in other places; no need to get 100% accurate. Thus approximating is easier and cheaper
- What's compositing?
	- Merging 2 pictures together

### Over Operator



# Week 7 Reading - Transfer Function

## Review on TF
- week 2
	- A transfer function defines colors at certain scalar values
- week 6 Volumetric Shading
	- The material values in Blinn-Phong Reflection Model are generated by TF

## STAR in TF
- A transfer function (TF) maps volumetric data to optical properties
- Curvature attribute for direct volume TFs
	- Interpreting isosurface curvature locally
		- curvature varies over the surface. As it's a quantity that is evaluated at a specific point, it is local in nature.
- 2-level volume rendoring
	- Different segmented objects can have different compositing modes.

# Segmentation
- Partitioning of image or data
	- Each partition is
		1. connected
		1. homogeneous
		1. have unique label
- Connectivity
	- How to determine if regions are connected?
		1. 4-connected neighbours (4 directions)
		1. 8-connected neighbours
		1. use 4-connected and 8-connected interchangably
			- e.g. 4-connectivity for foreground and 8-connectivity for background
- Homogeneous regions
	- The values of each pixel are consistent with having been generated by a mechanism
	- e.g. same color, brightness, motion, texture
- Segmentation Methods
	1. Threshold-based
		- simple
		- guarantee to form a closed region
	1. Region-based
		- clustering
	1. Hybrid methods
		- e.g. water flooding

## Thresholding
- Binary labeling
	- Idea: replace each pixel in an image with a black pixel if the pixel intensity is less than some fixed constant
	- Features
		1. Great for segmenting single object / same type objects
		1. Support notion of background and foreground
	- Real-world problems
		1. No uniform illumination
		1. Imaging devices are typically not equally sensitive across their field of view
- The above problems can be addressed by `local thresholding`
	- divide up the image into sub-domains and have a separate threshold for each sub-domain. 
	- Ways to do local thresholding
		1. Use different thresholds for each blocks
		1. Moving window

### Computing thresholds
- In general, the selection of thresholds need to be automated
- Approaches
	1. Simple way: if lots of constrast, then $$ T = i_{avg} + \epsilon $$ i.e. average value + noise
	1. Better way: histogram analysis
		- Try fitting Gaussians to the histogram
	1. Even better: **Otsu's method**
		- The threshold should minimize the sum of individual variances of the 2 distributions
			- i.e. minimize intra-class intensity variance
			- Searches for the threshold t which minimizes $$ \sigma^2_w(t) = w_0(t)\sigma^2_0(t) + w_1(t)\sigma^2_1(t) $$ where w is weight and $$ \sigma^2 $$ is variance. 
				- Weights can be computed from $$ w_0(t) = \sum_{i=0}^{t-1}p(i) $$
		- The other way to think about is to maximize the variance of the whole distribution respect to the cut (treating the threshold as the expectation / avg for calculating variance)
			- i.e. find t maxing $$ w_0(t)w_1(t)[\mu_0(t) - \mu_1(t)]^2 $$


## Segmenting multiple objects
- Otsu's method limitations
	1. Identify multiple objects of the same type (fail to assign unique labels)
	1. Identify multiple kinds of objects
- Alternatives
	1. Expectation Maximization
	1. Recursive Region Growing
	1. Graph-Theoretic Image Segmentation



## Graph Cuts
- Another approach of segmentation
- A graph cut formally is a set of edges that we delete from the graph that separates the graph into two connected components
- Images are represented as graphs
	- Each pixel is a node
	- Each pair of pixels has an edge
- Affinity
	- Will be used as weights to determine if an edge needs to be removed
	- Can be thought of as similarity
	- $$ affinity(p_i, p_j) = e^{-\frac{(f(p_i) - f(p_j))^2}{2\sigma^2}} $$
		- f is a distance function e.g. location, color, intensity
		- $$ \sigma^2 $$ is a normalization factor

### Cut
- Cut: set of links whose removal makes a graph disconnected
- Cost of cut: sum of the edge weights of the edges being cut
- To find minimum cut
	- Will focus on CO algorithm

### Interactive graph cuts
- A specific algorithm that uses user input to separate out a foreground object from a background
- User provides hints for alg to recognize foreground color
	- The red and blue lines on the images are user inputs indicating foreground versus background.
- Algorithm
	- Graph construction: adding 2 nodes which are not based on pixels to faciliate max flow (max bandwidth within each graphs) alg
		1. Add source s vertex (object terminal) and connect to all pixels
		1. Add sink t vertex (background terminal) and connect to all pixels
	- Use max flow (equal to min cut) algorithm to compute the cut
		- Foreground affinity (edge weight) from scribble pixel to s = infinity (max float achieves, so keeping the foregound pixel with s)
		- Foreground affinity from scribble pixel to t = 0
		- Background affinity from scribble to s = 0
		- Background affinity from scribble to t = infinity
		- Foreground affinity from non-scribble to s:  $$ affinity_{foreground}(p) = -\lambda log P_B(p) $$ where $$\lambda$$ is the weighting constant (a parameter for user to set), $$P_B(p)$$ is the probability that p is in the background [0, 1]
			- When p is a background pixel, then $$P_B(p) = 1 $$, then based on the formula, the foreground affinity (edge weight) would be 0, which makes sense 
			- How to compute the probabilities on whether a pixel is background / foreground?
				- Can use intensity of foreground pixels and compute histogram
				- Fit a Gaussian to the histogram and then use the distribution to compute $$ P_F(p) = P_F(I_p) $$
				- Given $$ P_F(p) $$, we can get $$ P_B(p) = 1 - P_F(p) $$
		- Background affinity from non-scribble to t: $$ affinity_{background}(p) = -\lambda log P_F(p) $$



# Transfer Functions
- TF is typically used on volumetric data
	- Volumetric data: a scalar function from a three-dimensional spatial domain with a one-dimensional range.
		- e.g. CT scan
- A TF defines 
	1. which parts of the data are essential to depict
	1. how to depict these portions of the volumetric data.
- Use cases
	1. Volumetric rendering - phong reflection model material
		- Two-level volume rendering
			- Different segmented objects can have different compositing modes.
	1. Segementation and classification: feature detection
- Challenge
	1. Come up with which transfer function to use
		- Need to look for discontinuity of the data to identify edges or surfaces
		- May need to segment multiple features
		- May need to create a MD (multi-dim) TF

## TF Taxonomy
- TF Categories
	1. Data-centric
		1. 1D TF
		1. MD TF

### 1D TF
- Operates on a scalar input value: maps scalar to visual representation (usually color and opacity)
- Most common form of TF
- Does not work well
	1. If there's significant overlap in intensity range
	1. If there's lots of noise

### MD TF
- Separable 2D TF
	- two separate 1D functions that are combined only after both 1D functions have been applied separately
- Non-separable 2D TF
- MD TF
- Dimensionality Reduction
	- Projecting high-dimensional TFs to a lower dimensional space to simplify the design process




# Glyphs (Week 8)

## Vector fields
- For a vector field, at every point in a given domain, there is going to be a vector component and magnitude defined.
	- i.e. a position and direction
- Vector visualization challenge
	- Have 1 pixel for 2 - 3 scalar values (magnitude + direction) to visualize

## Glyph intro
- Glyphs are icons that associates a symbol with a vector (a visual representation of vector)
	- e.g. spheres, rectangles
- General guide
	1. Need to sub sample the data
		- To avoid cluster
	1. Need to avoid uniform sub-sampling; instead, random / quasi-uniform sampling should be used

### Simple glyph: line segment (i.e. hedgehog plot)
- Idea
	1. Draw lines of $$ (x, x + kv(x)) $$
	1. Optionally color map with $$||v||$$
- It is acceptable to apply a scaling factor to a line segment glyph that represents a vector.
	- More scaling: easier to see high-speed areas, but more cluster
- Problem: high speed areas would have higher velocity and longer lengths of vectors. To reduce the impact of occlusion and better visualize high speed areas in flows,
	1. use fewer samples
	1. use line scaling to decrease line length

### Other 2D Glyph
- Cones and arrows
	- Advantages
		1. show orientation better than lines
		1. can utilize shading to help separate overlapping glyphs
	- Drawbacks
		1. takes more space

## 3D Visualization
- 3D Problems
	1. More data, same screen space - cluster
	1. Occlusion - overlapping
	1. Perspective foreshortening
	1. Poor viewpoint selection
- Alpha blending 
	- Set opacity of glyphs to a constant value < 1
	- Helps reduce perceived occlusion
		- High speed area would tend to be more opaque
- When using alpha compositing on vector field glyphs, high-speed areas tend to be more opaque.

## Problems
1. Hard for users to visually interpolate across a domain
1. A glyph takes more space than a pixel
1. Sub-sampling may be hard to perform - don't know how sparsely the resampling is needed



# Derived Quantities
- One other approach to vector field visualization is to visualize derived quantities of the vector field.

## Divergence
- A derived scalar value: $$ div = \frac{dv_x}{dx} + \frac{dv_y}{dy} + \frac{dv_z}{dz} $$ (i.e. sum of partial derivatives)
- Interpretation of divergence
	1. If div > 0, then there are more outgoing surrounding vectors from vector v
	1. If div = 0, then all surrounding vectors are passing through v towards the same direction
- Gives a good impression of where the flow enters and exits certain domain

## Vorticity
- Describes the local spinning motion of a continuum near some points
- $$ rot v = (\frac{dv_z}{dy} - \frac{dv_z}{dy}, \frac{dv_x}{dz} - \frac{dv_z}{dx}, \frac{dv_y}{dx} - \frac{dv_x}{dy}) $$
- Interpretation
	1. If $$||rot v|| > 0$$, then there's rotational flow
	1. If rot v == 0, then all urrounding vectors near v are towards the same direction
- Useful for flow simulation. e.g. aerodynamics, hydrodynamics


# Numerical Methods (Week 9)

## Euler's Method
- Used to solve (approximate the solution for) ODE (ordinary differential equation)
- $$ X_{n+1} = X_n + hv(X_n, t_n) + O(h^2) $$ where h is the step size, $$ v(X_n, t_n) $$ is the vector thatâ€™s sampled at $$X_n$$ at time $$t_n$$, and $$ O(h^2) $$ is the error that's inherited in the computation
- Sources of $$ O(h^2) $$
	1. Rounding error
	1. Truncation error (dominant)
		- Local error
			- error in one step
		- Global error
			- the cumulative overall error
			- Sum of local errors
- Reducing the step-size will typically reduce truncation error but may increase rounding error
- A method is first-order accurate when the local error is order $$h^2$$.


## Numerical Differentiation
- Differentiation is the process of finding the derivative of a function
- The derivative expresses the rate of change of an output with respect to a change in the input
- Centered Difference Formula: $$ f'(x) = \frac{f(X+h) - f(X-h)}{2h} $$


## RK (Runge-Kutta) Methods
- Solve ODE numerically and give tabluar result
- Euler's Method can be thought of as the 1st order RK method
- 2nd order RK method: Heun's 2nd order method
	- Local truncation error: $$ O(h^3) $$
- RK-4
	- Local truncation error: $$ O(h^5) $$


## Sampling
- Low discrepancy
	- Uniformly distributed
- Well distributed
	- Uniform distribution
	- Projection into 1D along x and y are also uniform
	- There's non-trivial minimum distance between all sample points
- Want sampling to be well distributed and not structured

### Random (RNG)
- Problems
	1. Too iregular
	1. Oversample some areas
	1. No minimal distance maintained among the points

### Jittered
- Generates a sample in each cell

### n-rooks
- Generates one sample exactly in each row and column
- Feature
	1. 2D distribution worse than jittered, so jittered can still be used
	1. 1D distribution better than jittered

### Multi-Jittered Sampling
- Use 2 grids
	1. Coarse grid: sqrt(n) * sqrt(n)
	1. Fine grid is n * n
- Analogy to soduko
- Features
	1. Great 1D projections and 2D distributions
	1. Used a lot in practice

### Hammersley Sampling
- A Quasi-random sequence
	- Features
		1. less random than random or pseudorandom sequences
		1. deterministic, just like a pseudorandom number generator 
		1. exhibit low discrepancy (i.e. more structural)
			- low discrepancy can be viewed as well distributed
- Radical inverse function of integer i to base 2: $$ \Phi_2(i) = \sum^n_{j=0}a_j(i) * 2^{-j-1} $$
	- Can be seen as converting the number to binary, then flip the bits, and then add the decimal point at the front
- Hammersley Sequence in 2D: $$ p_i = (x_i, y_i) = [i/n, \Phi_2(i)]$$
- Issues
	1. Too regular (too much structure)
	1. Only 1 sequence exists for a given n
	1. Need to recompute the whole sequence if adding 1 additional point

### Halton
- the Halton sequence for some n-dimensional domain that we want a sample is defined as taking the radical inverse base of some prime of i along each of the dimensions that we want to sample.
- Better than hammersley
- $$ p_i = (\Phi_2(i), \Phi_3(i), \Phi_5(i)) $$
- Example
	- The 2,3 halton sequence: 
		1. i = 1: p = (0.1 in base 2, 0.1 in base 3)
		1. i = 2: p = (0.01 in base 2, 0.2 in base 3)
		1. i = 3: p = (0.11 in base 2, 0.01 in base 3)
- Features
	1. Can easily add in additional points


### Poisson Disk Sampling
- Features
	1. Assure a minimum distance
	1. Generalizes well in high dimensions
- Steps 
	1. Given r - the minimum distance between 2 samele points
	1. Draw grids where the cell size = $$ \frac{r}{\sqrt{n}} $$ where dimensional space is n
	1. Generate the initial point p0 in a cell
	1. Draw circles with radius r from the 4 corners of the cell containing p0
	1. Randomly generate points and discard them if they are too far (further than 2r from center) from circle or within the circle 
- Issues
	1. Discrepancy is not low





# Visualization Techniques

## Displacement Plots
- Sparse VF visualization technique
- They are ineffective if the vector field is tangent to the displacement surface.


## Line Integral Convolution
- LIC is a dense visualization method for 2D flow visualization
	- Hard to do a dense visualization for flow in 3D
- Dense visualization method
	- use all the pixels in the display available to you
	- uncommon for flow visualization
- $$ T(p) = \frac{\int^L_{-L}N(S(p,s))k(s)ds}{\int^L_{-L}k(s)ds} $$ where
	- T refers to The output LIC texture
	- p refers to A pixel in the texture
	- N refers to The input noise texture defined over the domain of the vector field
	- k refers to The weighting function
	- L refers to The maximal distance of the streamline in the forward or backward direction
	- S refers to A streamline passing through the pixel p
	- $$ N(S(p,s)) $$: grey level of noise texture on the point s of the streamline passing through p
	- The whole expression is the weighted sum of grey values divided by the sum of weights  


### Algorithm
1. For each pixel p of the screen image
	1. Take the vector field and generate a streamline that is centered on that pixel
	1. Trace it upfield and downfield along the stream line, and we are going to sample the colors in the noise image as we go.
	1. Take an average of the grey values that the stream line is going through
	1. Assign the average to the pixel p


## Stream Objects
- Stream objects are referred to a set of visualization techniques for vector fields in which we drop particles into the vector field and trace out the path that they take as they flow along with the field
- Streamlines
	- The traces that we generate using the particle-based technique for a steady state vector field

### Stream Line

### Stream Tube

### Stream Ribbon


### Unsteady Flows
1. Pathline
	- A pathline is the unsteady analog of the streamline.
	- Steps
		1. We insert a single particle and then generate new positions across a bunch of time steps
		1. Draw a line connecting all of those positions of the particle throughout time in the unsteady flow then we have a pathline.
1. Timeline
	- Release multiple particles at one time and connecting the particles at each timestamps
1. Streakline
	- Release particles one-by-one and connecting the particles


# Tensor Visualization
## What are tensors
- Can be thought of as a machine that take in some number of vectors and then spit out some other number of vectors, but the output vectors are a linear function of the input vectors.
	- e.g. dot product can be viewed as a metric tensor
		- A metric tensor allows defining distances and angles near each point of a surface
- Definition: a tensor is an algebraic object that describes a (multilinear) relationship between sets of algebraic objects related to a vector space.
	- The relationship needs to be a linear mapping / linear transformation. i.e. a mapping V -> W between 2 vector spaces preserves vector addition and scalar multiplication
- Can also be thought of as a generalized matrix
	- Because tensor can be represented via a multi-dimensional array of numbers
	- A rank 2 tensor is a matrix (2-D array of numbers)
- Thinking from perspective of the information passed from rank-2 tensors
	- Tensors can show the variation of magnitude (at some signal at some point in space)
- Rank 2 tensors can also be viewed as a function
	- at $$ x \in R^3 $$ and in a direction $$ v \in R^3 $$, measure some magnitude $$ s \in R $$

## Curvature
- Will focus on rank-2 tensors with 3x3 matrices
- 1D Curvature
	- Curvature is the 2nd derivative of function f i.e. $$ C(x) = \frac{\partial^2f}{\partial{x}^2} $$
- 2D Curvature
	- Here 2D does not mean planar; instead, it means we have 2 variables x and y
	- $$ S \in R^3 $$
		- can write the z coordinates as a function of the XY coordinates i.e. $$ z = f(x,y) $$
		- thus surface which can be represented by $$ z = f(x, y) $$ 
	- The curvature would be specific for a point x0 in direction s
		- describes how quickly the normal $$n_s$$ changes around x0
	- $$ C(x, s) = \frac{\partial^2f(x)}{\partial{s}^2} $$ for any direction s
	- $$ C(x_0, s) = \frac{\partial^2f(x)}{\partial{s}^2}(x_0) = s^THs $$
		- H is called the Hessian of f 
			- $$ H = \begin{bmatrix}
					 \frac{\partial^2f}{\partial x^2} & \frac{\partial^2f}{\partial x \partial y}  \\
					 \frac{\partial^2f}{\partial y \partial x} & \frac{\partial^2f}{\partial y^2}
		 		 	\end{bmatrix}$$
	 	- Given H, can compute the curvature tensor at any point in any direction
 	- Constraints
 		- Need to construct local coord system (i.e. represent z in terms of x and y), which is hard to do

### 3D curvature
- General solution for functional surface
- Describe S as an implicit function i.e. f would be a function of x, y, and z
- $$ C(x0, s) = \frac{\partial^2f(x)}{\partial{s}^2}(x0) = s^THs $$ where H is Hessian of f, x0 is the point on the surface, and s is the direction vector
	- $$ H = 	\begin{bmatrix}
				 \frac{\partial^2f}{\partial x^2} & \frac{\partial^2f}{\partial x \partial y} & \frac{\partial^2f}{\partial x \partial z} \\
				 \frac{\partial^2f}{\partial y \partial x} & \frac{\partial^2f}{\partial y^2} & \frac{\partial^2f}{\partial y \partial z} \\
				 \frac{\partial^2f}{\partial z \partial x} & \frac{\partial^2f}{\partial z \partial y} & \frac{\partial^2f}{\partial z^2}
	 		 	\end{bmatrix}$$

## Diffusion Tensor
- Diffusion
	- the movement of individual molecules of a substance through a semipermeable barrier from an area of higher concentration to an area of lower concentration
- The dataset is typically a volumetric data generated by MRI
- anisotropic material
	- The properties of material varies with directions
- The diffusion tensor can take a point and a direction and tell us the speed of water motion in the brain tissue at a point in a direction.
- $$ D(x, s) = \frac{\partial^2f(x)}{\partial{s}^2} $$, where D is the diffusivity at a point x in direction s

### Simple Visualization
- Take slices of interest and visualize each slices
- Problem
	- Hard to identify the directions with strong/weak diffusion

### PCA (Principal Component Analysis)
- Can be used to identify the directions with strong/weak diffusion or steep/flat curves
- Steps for 2x2 Hessian (i.e. 2D - x, y)
	1. Fix a point x0
	1. calculate tangents (D(x0,s) or C(x0, s)) in all directions s at x0 to find a tangent plane
	1. Denote $$ \alpha $$ as angle of s respects to local coord axis x0
		- The angle would be on the surface of the plane (around the normal)
		- This allows describing a direction in that tangent plane by parametric vector that is parameterized using alpha 
	1. $$ \frac{\partial^2f}{\partial{s}^2} = s^THs = h_{11}cos^2\alpha + (h_{12} + h_{21})sin\alpha * cos\alpha + h_{22}cos^2\alpha $$
	1. Our curvature (as function of alpha) is extremal when $$ \frac{\partial{C}}{\partial{\alpha}} = 0 $$
		- This is equivalent to $$ Hs = \lambda * s <=> (H - \lambda * I)s = 0 $$
		- Thus non-trivial solution s != 0 means $$ det(H - \lambda * I) = (h_{11} - \lambda)(h_{22} - \lambda) - h_{12}h_{21} = 0$$
		- The 2 $$\lambda$$ are eigenvalues (i.e. principal values) of the tensor, and the 2 corresponding eigenvectors are the principle directions
			- Eigenvectors correspond to extremal (minimal / maximal) directions, and the eigenvalues are the actual minimal / maximal values
- 3x3 Hessian
	- 3 eigenvalue - eigenvector pairs
	- Max eigenvalue -> strongest diffusion / steepest curve
- Options to use PCA for visualization
	1. Mean diffusivity $$ \mu = \frac{1}{3}(\lambda_1 + \lambda_2 + \lambda_3) $$
	1. Ways to identify fibers in a domain
		1. Linear diffusivity: $$ c_l = \frac{\lambda_1 - \lambda_2}{\lambda_1 + \lambda_2 + \lambda_3} $$
		1. Fractional anisotropy: $$ FA = \sqrt{\frac{3}{2}}\frac{\sqrt{\sum^3_{i=1}(\lambda_i - \mu)^2}}{\lambda^2_1 + \lambda^2_2 + \lambda^2_3} $$ where $$ \mu = \frac{1}{3}(\lambda_1 + \lambda_2 + \lambda_3) $$
		1. Relative anisotropy: $$ RA = \sqrt{\frac{3}{2}}\frac{\sqrt{\sum^3_{i=1}(\lambda_i - \mu)^2}}{\lambda_1 + \lambda_2 + \lambda_3} $$
- Directional color coding
	- With PCA we find the direction with largest change (here expressed as eigenvalues). This direction will dominate the overall direction of the fiber and hence decide if will predominantly look red, green or blue. So you pick the vector in the direction of largest change.
		- e.g. R = |e1 * x|, G = |e1 * y|, B = |e1 * z| where e1 is the major eigenvector
- Shapes used for visualization
	1. Vector glyphs / hedgehogs
		- Seed only points where metrics are large enough (to visualize tissues)
		- Problems
			1. Occlusion
			1. Only uses major eigenvector
	1. Vector PCA
		- No glyphs drawn
		- No occlusion
		- Problems
			1. Only uses major eigenvector
	1. Tensor Glyphs
		- Ellipsoid glyph
			- Superquadrics are used must often among others



# Fiber Tracking (Tractography)
- Limitation of DT-MRI (Diffusion tensor MRI): do not attempt to recover the underlying anatomical structures (fiber tracts / axons)
- Use cases for fiber tracking
	1. quantitative comparisons of specific white matter pathways in disease
	1. show different pathways in congenital disorders or following recovery
	1. gain insights into normal brain anatomy
- Steps
	1. Identify seeds in region of high anistropy (fiber location) from PCA
		- Via linear diffusivity: $$ c_l = \frac{\lambda_1 - \lambda_2}{\lambda_1 + \lambda_2 + \lambda_3} $$
	1. trace streamlines in the direction of major eigenvector from PCA
		- Via trilinear component-wise tensor interpolation to reconstruct continuous tensor field
	1. Stop when anistropy is low

## Moving Least Squares
- Filtering alg to reduce noise
- Sliding window to try to fit a low degree polynomial by using least squares
	- Idea of least squares: minimizes sum of the squared error between expected value and actual value


# Tree Visualization

## Rooted Layout
- Start from root at the top
- Features
	1. Intuitive to see children and depth
	1. Can include extra information with different colors / shapes
- Issues
	1. Unbalanced aspect ratio can occur - scalability
		- A lot of extra (unused) space in between the branches

## Directed Acyclic Graphs (DAGs)
- Looks very like trees, so we can use rooted layout
	- One difference: we can have edge crossings in DAG but we should try to minimize

## Radial node-link tree
- Steps
	1. layout root at the middle
	1. layout the next level around a circle of the current level. The proportion of the arc depends on the number of subtree
- Features
	1. Level and sibling info can be easily captured
	1. More scalable than rooted layout (up to 10K nodes)
- Issues
	1. Nodes close to root get less space

## Bubble Layout
- Each subtree would have its own cycle, where the center is the root of the subtree
- Features
	1. Better spreading of the nodes for large trees
- Issues
	1. Hard to identify the depth in the tree

## Cone Tree
- Like a combination of bubble layout and rooted layout; in 3D
- Turned out to be not a good idea
- Features
	1. Effective in using 3D space
- Issues
	1. 3D is tricky - occlusions, perspective shortening
	1. Navigation was not helpful


## Force-Directed Layout
- Treat edges as springs, which exert attractive forces
- Treat nodes as atoms, which exert repulsive forces (electrostatic) forces to other nodes
- FR alg Steps
	1. Calculate $$k = C\sqrt(\frac{area}{number of vertices})$$
	1. For each iteration
		1. For every nodes:
			1. Calculate all attractive forces from its neighbours
				- $$ F_a = \frac{d^2}{k} $$
			1. Calculate all repulsive forces from all other nodes
				-  $$ F_r = \frac{k^2}{d} $$
- Advantages
	1. Simple to implement
	1. Work on any graph
	1. Interactive
	1. Effective
- Issues
	1. Runtime slow: $$ O(n^3) $$
		- Can be sped up using spatial partitioning and calculating Fr from only near-by nodes
		- Can employ multi-level computation
	2. Visual limit: 10k
	1. Layout may get trapped in local optimal instead of global optimal



# Large Graph Visualization
- Graph Preprocessing
- Clique
	- Completely connected subgraph
		- i.e. every vertex in the clique has an edge to each other vertex in the clique
- Betweeness Centrality (BC)
	- Vertice: $$ g(v) = \sum_{s \not= v \not= t} \frac{\sigma_{st}(v)}{\sigma_{st}} $$ where $$ \sigma_{st} $$ is the total number of shortest paths from node s to node t, and $$ \sigma_{st}(v) $$ is the number of those paths that pass through node v
	- Edge: $$ g(e) = \sum_{s \not= t} \frac{\sigma_{st}(e)}{\sigma_{st}} $$
		- $$ \sigma_{st}(e) $$ is the number of those paths that pass through edge e

## Edge Filtering
- Works well for **scale free graphs**
	- the fraction of nodes with degree k follows a power law $$ k^{-\alpha} $$
		- i.e. The probability of finding a node with degree k decreases exponentially when k increases
	- e.g. a social network is thought to be weak scale-free
- Steps
	1. Sort by edge BC and remove the edges that have BC under a threshold starting from the lowest BC
	1. Recover Connectivity
		1. Iterate through the removed edges from high BC to low
		1. Restore the edge if it connects 2 different components
	1. Recompute the layout (typically force-directed layout)
- BC is the bottleneck for performance
	- Can be approximated via algs
- Limitation
	1. Does not work well for non-power law graphs e.g. planar graph

## Edge Bundling
- A technique for graph aggregation
	- Produces a simpler / smaller 'cluster graph' from a large one
	- Can be used not only on trees
- Idea
	- Remove high-BC edges to discover clusters

### Hierarchical Edge Bundling
- Intended for graphs with relatively few vertices that have many neighbours
	- The algorithm assumes social networks are separated into communities (small heavily connected subgraphs) with few edges crossing between them. If the whole graph is heavily connected, the algorithm will not be able to effectively prevent edge crossings in the layout.
	- Does not have to be trees
- A hierarchy node may contain multiple nodes from the original graph
- All of the vertices of the graph are laid out around the perimeter of the circle
- The constructed vertex hierarchy is used to generate control points
	- Control points would control the directions of the parametric curves
- Community Discovery
	- Will remove high BC edges to discover communities
		- Low BC edges - within a community
		- High BC edges - connect different communities 
- Balanced Hierarchy Construction
	- Steps
		1. Filter edges (removes edges in BC order from high to low)
		1. Create subtrees
		1. Merge communities to a single hierarchy
			- Merge from lowest BC edges
		1. Generate hierarchical edge bundles using the hierarchy graph
- Rebuild the graph
	- Steps
		1. Place low BC edges on perimeter
		1. Rebuild tree by connecting in order of increasing BC




# Mesh Simplification
- A technique for dealing with really large amounts of surface mesh data
- A tessellation is a covering of a surface with geometric shapes
	- A surface mesh is a tessellation of the true surface
- Overtesselated
	- Use more meshes than needed

## Vertex Clustering
- Steps
	1. Cluster generation
		- Approaches
			1. Use uniform grid. The vertices fall into the same cell would be clustered
			1. Hierarchical approach
			1. Top-down / Bottom-up
	1. Compute a representative vertex (within each cluster)
		- Metrics
			1. Avg vertex pos
				- Not effective in dealing with sharp edges
			1. Median vertex pos
				- A vertex of the original mesh closest to the average position of the vertices in the cluster.
				- Better than the avg vertex pos
			1. Error quadrics
				- Better than avg / median
				- Tries to minimize the sum distances of vertex' planes
					- $$ p = [x, y, z, 1]^T, q = (a, b, c, d)^T $$ where a b c d are coefficients of the plane (i.e. $$ax+by+cz = 0$$), Individual $$ Q_i = qq^T $$
					- $$ \sum_idist(q_i, p)^2 = p^TQ_pp $$
						- $$Q_p$$ is the sum of all of the 4x4 matrices that are associated with each one of those points. i.e. $$ Q_p = \sum_iQ_i$$
					- Once we have computed $$Q_p$$, the rest of the computations are very fast regardless how many planes there are
						- $$ Q_pp $$ consists of 16 scalar multiplications, and $$p^T(result of Q_pp) $$ consists of 4 scalar multiplications
					- To minimize the sum distances, we can solve the system of linear equations of $$Q_pp = [0,0,0,1]^T $$ to get p
	1. Re-generate mesh
		- Create edges between the representatives
	1. May need to handle topology changes
		- Disconnected components may have vertices included in a cell. For such cases, the components will be connected after clustering
- Since sheets will be combined via a single vertex, it would no longer be manifold after clustering
	- Manifold: A topological space that locally resembles euclidean space near each point

## Edge Collapse
- Will not introduce topology changes
- Idea
	- Can collapse 2 vertices and combine those into a single one
- Can use error quadrics to determine the new combined vertex
	- Can do a few iterations and in each iteration pick the ones with lowest error quadrics to perform edge collapse
- Though slower, edge collapse is in general better than vertex clustering





