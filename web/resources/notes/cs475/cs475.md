# CS 475

## Lecture 1 (2018-05-02)
### Prerequisites
- Basic linear algebra and calculus
- Scientific programming (Matlab, Python, Julia)
- Mathematical Maturity

### Assignments
- A1: 05-02 - 05-23 (10%)
- A2: 05-23 - 06-11 (10%)
- A3: 06-13 - 07-04 (10%)
- A4: 07-04 - 07-23 (10%)
- Always acknowledge the source of help (e.g. google, stackoverflow)
- No late submissions

### Exams
- Midterm: 06-13 10:00 - 11:20 MC 2065 20% (open book)
- Final: 40% (open book)

### Outline
- Matrix Decompositions
	- LU. Gaussion Elimination
	- Cholesky
	- PDE (partial differiential equation)
	- Graph representation and ordering
	- Inverse problems
- Iterative method
	- Splitting
	- Steepset descent
	- Leaset-squares
	- Conjugate greadient
	- Projections
- Spectral Decomposition
	- QR
	- Reflection & Rotations
	- Eigen-value
	- Rayleigh Quotient
	- Subspace iteration
	- SVD

### Applications
- Page Rank (Priority of a search result)
	- top 10 algs. in science & engineering for 20th century
		- Metropolis alg. for Monte Carlo (covered)
		- Simplex
		- Krylor Subspace (covered)
		- Decomposition algs (covered)
		- Fortran (covered)
		- QR (covered)
		- Quicksort
		- FFT (fast fourier transform)
		- Integer relation detection
		- Fast Muti-pole alg.
	- Assume n webpages on the internet, pick $$ S_{0} $$ = j webpage
		- toss a coin at time t
		- head: click a link on webpage $$ S_{t} $$, go to $$ S_{t+1} $$. If no link on $$ S_{t} $$, restart from $$ S_{0} $$
		- tail: gp to  $$ S_{t+1} = k w.p. v_{k} $$
		- in the long run, how often am I on webpage k? (this is how google calculate pagerank)
	- Math
		- Initial distribution
		- Transistion Matrix $$ P_{ij} = P_{r}(S_{t+1} = i | S_{t} = j) $$
			- column-stockestic matrix
		- Teleportation 
		- Biased Coin
		- Define $$ x_{t}(i) $$ as the prob of being on webpage i at time t, $$ \vec x_{t+1} = \alpha * P * \vec x_{t} + (1 - \alpha) * \vec V = [\alpha P + (1 - \alpha)\vec v 1^T ]\bar P * \vec x_{t} $$. $$ \vec x_{t} $$ is a prob vector that sums up to 1 i.e. $$ \vec x^T1 = 1 $$
	- Ergodic Theorem: $$ \frac{1}{T} \sum_{t=1}^{T}1(S_{t} = i) -> x(i) $$ where $$ \vec x = \bar P * \vec x $$
		- \vec x is an eigenvector of P, with eigenvalue 1
	- $$ (I - \alpha * \bar P) * \vec x = (1 - \alpha) * \vec v $$
	- $$ x = \bar P * x $$
- Semi-supervised learning / dimensionality reduction
- Graph-cut
- Information retrieval