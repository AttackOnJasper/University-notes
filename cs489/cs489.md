# CS 489 (Spring 2018)

## Lecture 1 (2018-05-02)
### Prerequisites
- Algorithm
- Mathematics: linear algebra, probability, calculus
- Programming: matlab, python, julia

### Overview
What is machine learning?  
Field of study that gives computers the ability to learn without being explicitly programmed

### 3 learning categories
- Supervised learning (data with labels specifying the correctness of the data)
	- Given a traning set of pairs of examples (x, y)
	- Return a hypothesis function h(x) -> y
		- y is continuous -> regression e.g. housing prices
		- y is discrete -> classification (finite set e.g. {0, 1, 2})
	- Regression
		- Need to divide the data into training set and testing set s.t. the hypothesis can be generalized
		- Need to determine input
			- number of features (too many features -> overfit, hard to generalize; too few features -> underfit)
			- what kind of function to choose (e.g. linear, quadratic)
		- e.g. movie rating prediction
			- input: movie name, output: movie learning
	- Classification
		- Examples
			- ImageNet: put images into classes (deep convolutional neural networks)
			- Email spam filtering
- Unsupervised learning
	- There is no output (or label, 'y') compared to supervised learning 
	- Find 'potential pattern' of the data
	- Clustering
		- e.g. K-means
			- determine the # of clusters
			- choose random points based on the # of clusters predicted
			- Associate data points with center points in step
		- e.g. Autoencoders (artificial neural network)
			- learn representation for a set of data
			- learn generative models of data
- Reinforcement learning (not covered in this course)

## Linear Algebra Review (GBC Chapter 2)
### Scalar, Vector, Matrix, Tensor
- Scalar: e.g. s $$ \in \Re $$
- Vector: e.g. **x** $$ \in \Re^n $$
	- ith element: $$x_i$$
	- $$ x_S $$ where S is {1,3,6} represents {$$ x_1, x_3, x_6 $$}
- Matrix $$ A \in \Re^{m*n} $$
	- element: $$ A_{i,j} $$, ith row: $$ A_{i,:} $$, jth column: $$ A_{:,j} $$
	- main diagonal
- Tensor: array with more than 2 axes. e.g. get specific element: $$ A_{i,j,k} $$
- Matrix + Vector: $$ C_{i,j} = A_{i,j} + b_j $$ (implicit copying of b to many locations is called broadcasting)

### Vectors, Matrices Multiplication
- Matrix product
- Hadamard product (element-wise product of 2 matrices)
- Dot product of 2 vectors
- System of linear equations $$ Ax = b $$ where $$ A \in \Re^{m*n}, x \in \Re^{n}, b \in \Re^{m} $$

### Special Matrices and Vectors
- Identity
- Inverse: should not be calculated explicitly in practice
- Diagonal
- Symmetric
- unit vector: vector with unit norms i.e. $$ ||x|| = 1  $$
- 2 vectors x,y are **orthogonal** if $$ x^Ty = 0 $$
	- 2 vectors are **orthonormal** if they are orthogonal and unit vectors
- Orthogonal matrix: $$ A^{-1} = A^T $$

### Linear dependence and Span
- Ax = b has a solution iff b is in the **column space** of A (span of A's column vectors)
- **Linearly independent** no vector in the set is a linear combination of other vectors
- **rank** of a matrix: maximum number of linearly independent column vectors

### Eigen Decomposition
- $$ Av = \lambda v $$ where v is an eigenvector and $$\lambda $$ is an eigenvalue
- **Characteristic equation** $$det(A - \lambda I) = 0 $$
	- $$\lambda $$ is an eigenvalue of $$A$$ iff it is a root of characteristic equation
		- proof:
			- if $$ \lambda $$ is an eigenvalue, then $$ (\lambda I - A)u = 0 $$ has a non-trivial solution
			- so $$ \lambda I - A $$ is singular -> $$ det(A - \lambda I) = 0 $$ -> $$ \lambda $$ is a root
- Upon similarity transform, eigenvalues do not change
- If $$ A \in \Re^{n*n} $$ is a square matrix with rank n, then $$ A = V \Lambda V^{-1} $$, where $$V_{:,i}$$ is ith eigenvector, and $$ \Lambda_{i,i} $$ is ith eigenvalue

### Singular Value Decomposition




## Probability Review (GBC Chapter 3)
### Why probability in machine learning?
- 3 possible sources of uncertainty
	- Inherent stochasticity in the system being modeled
	- Incomplete observability
	- Incomplete modeling
- Frequentist probability: rates at which events occur
- Bayesian probability: qualitative level of certainty
	- Bayesian statistics: the evidence about the true state of the world is expressed in terms of **degrees of belief** known as Bayesian probabilities

### Definitions
- **Random variable** a variable that take on different values randomly
- **Probability distribution** description of how likely a random variable or set of random variables is to take on each of its possible states
- **probability mass function** A probability distribution over discrete variables
	- maps from a state of a random variable to the probability of that random variable taking on that state e.g. $$ P(X = x) $$
	- domain of P must contain all possible states of the random variable
- **Probability density function** A probability distribution over continuous variables
	- A probability density functionp(x) does not give the probability of a specific state directly; instead the probability of landing inside an infinitesimal region withvolume δx is given by p(x)δx.
	- $$ \int p(x)dx = 1 $$
- **joint probability distribution** a probability distribution over many variables e.g. $$ P(X = x, Y = y) $$
	- probability mass functions that act on many variables at the same time
- uniform distribution: $$ P(x = x_i) = \frac{1}{k} $$ where i = 1 ... k
- **Marginal probability distribution**

### Conditional Probabilities
- Bayes' rule (Probability of an event given that another event happens)
	- $$ P(Y = y | X = x) = \frac{P(Y = y, X = x)}{P(X = x)} $$
- Chain rule (general product rule) $$ P(A,B) = P(B|A)*P(A) $$, where P(A,B) is joint, P(B|A) is conditional, and P(A) is marginal


## Calculus Review
- **Partial derivative** of a function of several variables is its derivative w.r.t. one of those variables (other variables are treated as constants)
- Matrix / vector differentiation
- **Jacobian matrix** matrix that contains all partial derivatives of a function whose input and output are both vectors
- **Hessian matrix** matrix contains all second derivatives; it's the jacobian of the gradient

## Optimization Review
### Notations
- **Objective function** The function we want to minimize or maximize
	- if minimizing, we may also call function as cost function, loss function, or error function


## Lecture 2
### Linear Algebra
- Norms (measure size of vector)
	- $$L^P $$ norm: $$ ||x||_p = (\sum_i|x_i|^p)^{\frac{1}{p}} $$
	- the norm of a vector x measures the distance from origin to x
	- L1-norm: **least absoluate deviations** (also least absolute errors)
		- $$ S = \sum^{n}_{i=1}|y_i - f(x_i)| $$
		- robust (resistant to outliers in the data), unstable solution, may have multiple solution, built-in feature selection
	- L2-norm: square root of sum of vectors (Euclidean norm)
		- size of vector: $$ ||x||_2 = sqrt(x^Tx) $$
		- **Least Squares** $$ S = \sum^{n}_{i=1}(y_i - f(x_i))^2 $$
		- when near the origin, L2-norm is undesirable because it increases very slowly
		- not very robust, stable solution, always one solution, no feature selection
	- If p is infinity, then $$ ||x||_p $$ is the maximum entry of x
- Positive semi-definite matrix: M is postive semi-definite if $$z^TMz, z^*Mz $$ must be positive or zero, where z is a random non-zero column vector, $$z^*$$ is the conjugate transpose of z
	- all eigenvalues are >= 0
	- can be decomposed as $$A^TA$$

### Point estimation (single estimation of a parameter)
- Prefer small bias and small variance
- Likelihood function $$ L(\theta | x) = p_{\theta}(X = x) $$
- Maximum likelihood (ML) method
	- Assume that data follows a distribution, maximize log-likelihood function to get $$\theta$$
	- Find an estimation of θ that **maximizes the probability of the observed data set** (most plausible $$\theta$$ based on observed data set)
	- How many samples do we need? use Hoedffding's inequality
	- e.g. Bernoulli distribution: estimate $$ P(x = 1) $$
	- e.g. Gaussian distribution: estimate $$ \mu, \sigma^2 $$
	- Limitation: data in practice is never generated from a distribution
- Maximum A Posteriori (MAP) method (Bayesian perspective)
	- **prior probability distribution** the probability distribution that would express one's beliefs about this quantity before some evidence is taken into account
	- **posterior probability** the conditional probability that is assigned after the relevant evidence or background is taken into account
	- Maximizes posterior distribution
	- can be seen as ML with regularization (prior term)

### Bias-variance tradeoff
Mean square error   
high bias -> underfitting; high variance -> overfitting

### Feature Scaling
- Standardization
	- centering
	- scaling
	- after this the data will have mean 0 and variance 1
- Normalization: add normalization term to change the weight of certain features
	- after this the data will be bounded within some range e.g. [0,1]



## Lecture 3, 4 Linear Regression
### Regression
- Learn (find) hypothesis function, a function maps input x to output y
- Try to minimize loss function
	- Least squares: $$ \underset{f}{\min} \sum[(f(X) - Y)^2] $$
	- $$ f(X) = E(Y|X) $$

### Linear regression
- $$ E(Y|X) = w^Tx + b $$, where w is linear coefficient
- Ideally we know the distribution, so we can get $$ J(w,b) = \underset{w,b}{\min}E[(w^Tx + b - y)^2] $$; however, in practice, we need to use empirical risk function: $$ J(w,b) =  \underset{w,b}{\min}\frac{1}{N}\sum[(w^Tx + b - y)^2] $$ (i.e. use sample mean to approximate mean)
	- Simplification: add b to w, and add 1 to x, then $$ J(w) = \underset{w \in \Re^{d+1}}{\min}||Xw-Y||_2^2 $$
- Normal equation method
	- $$ J(w) = \underset{w \in \Re^{d+1}}{\min}||Xw-Y||_2^2 = w^T(X^TX)w - 2w^TX^Ty + y^Ty$$
	- then take derivative, we get $$X^TXw = X^Ty$$, then we can solve the linear system to get w
- Gradient descent method 
	- $$w = w - \alpha\frac{\partial J(w)}{\partial w} $$
	- $$ \frac{\partial J(w)}{\partial w} = 2X^TXw - 2X^Ty $$
	- Need many iterations
	- Work well when number of features is large
- After getting w and b, we can use test set to evaluate the learned hypothesis

### Generalization
We can do non-linear regression by using linear regression model: construct non-linear basis function & convert hypothesis space to linear

### Regularization
- Overfitting: too many number of features compared to number of training data
	- Soln: 
		- reduce number of features (hard to determine which feature to remove)
		- regularization: change weight of features
- Ridge regression
- Pre-process input and output can eliminate intercept b (Center & standardize)
- $$ J(w) = \underset{w}{\min}\frac{1}{N}\sum^N_{i=1}(w^Tx_i - y_i)^2 + \lambda||w||^2_2 $$
- Gradient descent: $$w = w(1 - 2\alpha\lambda) - \frac{\alpha}{N}(2X^TXw - 2X^Ty) $$
- Lasso: use L1 norm for regularization term instead of L2 norm

### Cross validation

## Lecture 5 KNN
### Classification
- K-NN: assign most frequent label among k-nearest neighbours
	- When k increases, training error increases; test error and CV error decreases then increases
- How to choose k? select k with the **lowest validation error** (cross validation)

### Issues and Improvements for KNN
- Data might be skewed (e.g. significantly more red balls than blue balls)
	- solution: assign larger weight to closer neighbours
- **Feature scaling** standardize features first so that each feature contributes approximately equally to (euclidean) distance metric
- **Dimension reduction**
- Changing distance metrics

## Lecture 6 Logistic Regression
### Bayes Classifier and classifer types
- zero-one loss function: $$ L(Y, \hat{Y}) = (Y == \hat{Y}) ? 0 : 1$$
- Expected prediction error (EPE) = $$ E(L(Y, \hat{Y})) $$
- error is minimized when posterior distribution $$ Pr(y_k | X) $$ is maximized for k = 1 ... K
- Bayes rule: $$ Pr(Y_k | X) = \frac{Pr(Y_k, X)}{Pr(X)} $$, where Pr(X) is evidence
- **Generative classifier** model joint distribution $$ Pr(Y_k, X) $$ under distributional assumptions
	- so $$ Pr(Y_k | X) = \frac{Pr(Y_k)Pr(X|Y_k)}{Pr(X)} $$
	- e.g. Naive Bayes: probabilistic classifier based on applying Bayes' theorem with assumptions that features are independent
		- e.g. a fruit may be an apple if it's red, round, and about 10 cm in diameter, a NB classifier considers those 3 features as independent
		- probability model: maximize posterior probability $$ Pr(Y_k | X) = Pr(Y_k)\prod_{j=1}^{d}{Pr(X_j|Y_k)} $$
	- e.g. Gaussian mixture models
- **Discriminative classifier** model conditional probability based on data w/o distributional assumptions
	- e.g. logistic regression, SVM

### Logistic Regression
- Sigmoid function; linear boundary $$w^Tx = 0$$
- Likelihood function
- Loss function: negative log likelihood function - (cross-entropy loss)
- Newton's method
- Regularization
- Multiclass classification
	- one vs one
	- one vs rest
	- direct multiclass classification

## Lecture 7 Perceptron
Activation functions

### Perceptron
Activation function: **sign** ``` sign(t) = (t >= 0) ? 1 : -1 ```   
Prediction: $$ \hat{y} = sign(\sum_{d=1}^D{w_dx_d + b}) $$   
Batch vs online   
Perceptron training alg. (online): error driven update

### Convergence
perceptron converges iff data is linearly separable   
Distance between x and hyperplane $$ w^Tx + b = 0 $$: $$ \frac{|w^T+b|}{||w||} $$   
Margin(D, w, b): minimum distance to one of the sides of D if w separates D   
- Max margin: have ~ same distance to both sides   

### Extensions
- Stochastic gradient descent: adjust weight based on only 1 example each time
- Multiclass perceptron
- Linearly non separable? Solutions:
	- find another feature representation
	- kernel methods
	- NN

## Lecture 8 Kernels
If the data set is not linearly separable, then perceptron might not converge.  
Solution: by adding new features (mapping the data to a larger feature space), the data might become linearly separable  
- Linear operation in the feature space is equivalent to non-linear operation in input
space (e.g. finding non-linear decision boundary)

### Kernels
**Feature Space Mapping** given input $$x \in \Re^d$$, a feature space mapping $$ \phi: R^d -> R^M $$  
**Input space** the space where x's are located  
**Feature space** the space of $$\phi(x)$$ after transformation  
e.g. $$ \phi: \Re -> \Re^3, \phi(x) = [1, x, x^2] $$  
**Kernel function** $$ k(x,x') = \phi(x)^T\phi(x') $$ ($$ k:\Re^d * \Re^d -> \Re $$)

### Kernel verification and construction
- To verify if a function k is kernel function
	1. find $$\phi$$ s.t. $$ k = \phi(x)^T\phi(x) $$
	2. Gram matrix K (where $$ K_{ij} = k(x_i, x_j) $$) is symmetric and positive semi-definite
- To construct k
	1. find $$\phi(x)$$
	2. construct from other kernels by rules that preserves p.s.d.

### Kernel trick
- Kernelized Ridge regression
	- use dual representations to eliminate $$\phi$$
	- prediction
	- complexity
		- primal solution i.e. use $$ \phi $$ - depends on number of basis functions M
		- dual solution i.e. use kernel trick to eliminate $$ \phi $$ - depends on number of training examples N

## Lecture 9 Gaussian Process
- side note: sampling (generate values) from a gaussian distribution - X ~ N(0,1) 
	- could draw + append to generate a vector of multivariate gaussian variables from gaussian distribution
### Gaussian Process
- Random variable
- Gaussian process is a function of 2 variables $$ Z(t,w) $$
- covariance function $$\leftrightarrows $$ kernel function

### Gaussian Processes for regression
- Gaussian LR
- Bayesian LR

### Gaussian Processes for classification


## Lecture 10 Mixture models and EM
### K-means
Problem: assign n examples to k clusters   
Objective: the sum of the squares of the distances of each data point to its closest vector $$\mu_k$$ is minimized   
	- need to find $$\mu_k $$, which are centers of k clusters, and $$r(n,k)$$, which indicates whether n-th example belongs to k-th cluster   
2-step optimization   
Hard assignment

### Misture of Gaussians

### EM (Expectation Maximization)
```
iterate:
	E step: evaluate responsibility term using current params
	M step: update current params using new responsibility
```

## Lecture 11 Hard-margin SVM
Assume data is linearly separable   
Prediction same as perceptron   
Find w s.t. the decision boundary is the one for which the margin is maximized   
Support vectors: points touching the margin boundary (only those are used for prediction)

### Optimization
Maximum margin is found by solving $$ \underset{w,b}{max}\{ \frac{\underset{n}{min}[y_n(w^T\phi(x_n) + b)]}{||w||}\} $$ (i.e. maximize the minimum distance between point x and hyperplane $$w^Tx + b$$)   
Assume w.l.o.g. $$ y_n(w^T\phi(x_n) + b) = 1 $$ for the point that is closest to boundary. Then margin is $$ \frac{1}{||w||} $$   
	- so max margin can be found by quadratic optimization problem

### Lagrange and Dual Problem
- Lagrange multiplier
- Primal problem (Linear problem)
	- for SVM: $$ \underset{w,b}{min} \frac{1}{2}||w||^2 $$ s.t. $$ y_n(w^T\phi(x_n) + b >= 1 $$, for all n
- Dual problem: incorporate constraint into objective
- Duality gap; zero duality gap
	- KKT conditions for zero duality
- Lagrange for SVM (assume $$ y_n(w^T\phi(x_n) _+ b) = 1 $$, which can always be achieved by sacling w and b)
	- $$ L(w,b,a) = \frac{1}{2}||w||^2 - \sum^N_{n=1}a_n[y_n(w^T\phi(x_n) + b) - 1] $$
	- Derive w and b
	- Derive a
- Convex set; Convex hull

### Regularization vs. Constraint
Can always transform from regularization form to constraint form

## Lecture 12 Soft-margin SVM
### Soft-margin SVM
- Allow data points on the 'wrong side'
- Primal Problem
- Lagrangian: $$ L(w,b,	\epsilon, a,\mu) $$
- Dual problem
	- derive a
	- derive b

### Relation to logistic regression
Zero-one loss   
Hinge loss

## Lecture 13
### Multi-layer perceptron
### Back-propagation

## Lecture 14 Training of Deep NNs
### Activation functions
* Methods to avoid vanishing gradients
	1. Proper initialization of network parameters to avoid saturated region in activation functions
	2. Choose activation functions that do not saturate e.g. ReLU, leaky ReLU, Maxout
	3. Use LSTM or GRU

### Regularization
1. Parameter norm penalties e.g. ridged regression
2. Bagging: combine several models
3. Dropout: ignoring some units (hidden / visible) during forward / backward pass in training phase
	- inverted dropout: keep the mean value of output unchanged
	- Use inverted dropout for each training example
	- How to determine hyperparameter p - probability of keeping units
		1. keep p same for all layers
		2. set a lower p for overfitting layer
		3. could also design p for input, but usually set p very close to 1
4. Data augmentation: create fake training data
5. Early stopping
6. Batch normalization (not directly used for regularization)
	- Normalize hidden layer values (either before or after activation)
	- Effect of BN
		1. each layer can be trained independently
		2. speed up
		3. regularization

### Gradient-based optimization
Momentum   
Exponentially weighted averages: recent gradient has more weight 

## Lecture 15 CNN
### Layers in CNN
- Fully-connected layer
- Convolutional layer
	- filter
		- local connectivity
	- hyperparameters
		- depth: number of filters
		- stride: how to slide the filters
		- zero-padding: pad input with zeros around the border
- **Pooling layer** progressively reduce the spatial size of the representation to reduce the amount of parameters and computation in the network
	- also control overfitting  
	- common types: max-pooling, average pooling, L2-norm pooling

### Parameter sharing
Need to control the number of parameters   
Idea: constrain the units in each depth slice to use the same weights and bias

## Lecture 16 RNN
- Parameter sharing in RNN: unit is produced using the same update rule applied to previous ones
- A character-level RNN reads words as a series of characters - outputting a prediction and “hidden state” at each step, feeding its previous hidden state into each next step

### Gradient problems
- Vanishing gradient
	- LSTM
	- GRU
- Exploding gradient
	- clipping

### Bidirectional RNNs
Motivation: need information from earlier timestamps as well as later timestamps

## Lecture 17 GAN (Generative Adversarial Networks)
- Generative Model
	- Density estimation
	- Sample generation (implicit density)

### GAN
- 2 player game
	- Generator network
		- Input: random noise
		- Goal: generate samples from data distribution and
	- Discriminator network
		- Input: real / generated images
		- Cost function
		- Goal: distinguish between real / generated images
- Minimax Theorem (originated for 2-player zero-sum game theory: total gain + total loss = 0)
	- **Minimax** minimize the possible loss for a worst case
	- **Maximin** maximize the minimum gain
- Tricks
	- class-conditional
	- one-sided label smoothing
	- batch normalization
- types
	- SRGAN
	- DCGAN
	- Wasserstein GAN
- pros
- cons

## Lecture 18 Auto-encoder
### Linear autoencoder
- loss function
- optimal pair of encoder and decoder

### nonlinear autoencoder
- Undercomplete autoencoders: bad generalization
- Overcomplete autoencoders: too much power; would just copy input to output
- Regularized autoencoders
	- Sparse autoencoder
		- goal: constrain hidden units to be inactive most of the time
		- sparsity penality
		- loss function
	- Contractive autoencoder
		- goal: encoder is resistant to small chages of input e.g. rotations
		- loss function
	- Denoising autoencoder
	- Variational autoencoders

## Lecture 19 Decision Tree
Idea: split training sets into subsets until subsets are pure   
How to choose a good attribute to split? 

### ID3 (Iterative Dichotomiser 3)  
- Goal: maximize information gain / minimize uncertainty   
- **Entropy** $$ - \sum_{i=1}^{n}p(x_i)log_bp(x_i) $$ (measures uncertainty, usually b = 2)   
- **Information Gain**
- properties
	- cannot ensure optimal solution
	- non robust
	- overfitting (could stop early / prune the tree)
	- hard to use for continous data (could use threhold split)
- Information Gain Problem: too much partition
	- use Gain ratio instead 

## Lecture 20 Ensemble Method
### AdaBoost
### Exponential Error function


## Colab
similar to jupitor notebook

## NumPy
Main object: homogeneous multidimensional array   
Dimensions == axes (e.g. 2-by-3 array -> 1st axis has length 2, and 2nd one has length 3)   


## Tensorflow

## Keras
a high-level neural networks API written in Python

## PyTouch
a deep learning framework; replacement of NumPy to use the power of GPUs   

### Basics
- Tensors: similar to NumPy's ndarray
	- Operations
		- torch.view(x, y): reshape tensor
		- torch.cat: concatenates the given sequence of seq tensors in the given dimension
- Autograd: automatic differentiation
	- Can track gradients of all operations on tensors
	- Gradient
- Neural Network (torch.nn package)
	- depends on autograd to define models and differentiate them
	- methods:
		- forward(input): output
		- backward (implemented by autograd)
	- steps:
		- define nn with some learnable parameters (weights)
		- feed input and calculate loss
		- update weights by rule: weight = weight - learning_rate * gradient
	- layers
		- convolution layers
		- pooling layers
		- padding layers
		- recurrent layers
		- linear layers
		- dropout layers
		- sparse layers



